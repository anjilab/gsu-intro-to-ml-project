{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.losses import KLDivergence, sparse_categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n"
      ],
      "metadata": {
        "id": "xGqzAgZwzE5S"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "ZEpiD4KQzAVW",
        "outputId": "26bdb1f9-912e-42da-8e16-b9bfb912e5fa"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Combine training and test datasets to get the full dataset\n",
        "x_full = np.concatenate((x_train, x_test), axis=0)\n",
        "y_full = np.concatenate((y_train, y_test), axis=0)\n",
        "\n",
        "\n",
        "# Function to select a specified number of samples for each category\n",
        "def select_samples(x, y, num_samples):\n",
        "    selected_samples = []\n",
        "    selected_labels = []\n",
        "    for i in range(10):  # 10 categories in MNIST\n",
        "        indices = np.where(y == i)[0][:num_samples]\n",
        "        selected_samples.append(x[indices])\n",
        "        selected_labels.append(y[indices])\n",
        "    selected_samples = np.concatenate(selected_samples, axis=0)\n",
        "    selected_labels = np.concatenate(selected_labels, axis=0)\n",
        "    return selected_samples, selected_labels\n",
        "\n",
        "# Select 1,000 samples for each category\n",
        "x_subset, y_subset = select_samples(x_full, y_full, 1000)\n",
        "\n",
        "# Display one random image from x_train_subset\n",
        "index = np.random.randint(0, len(x_subset))\n",
        "image = x_subset[index]\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.title(f\"Label: {y_subset[index]}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing data set i.e 100 labeled datas i.e 10 labeled from each category.\n",
        "np.random.seed(0)\n",
        "labeled_mask = np.zeros(10000, dtype=bool)\n",
        "for i in range(10):\n",
        "    indices = np.where(np.array(y_subset) == i)[0] # returns shuffled array of indices of each class, total 1000 data.\n",
        "    np.random.shuffle(indices)\n",
        "    labeled_mask[indices[:10]] = True\n",
        "\n",
        "\n",
        "# Get the index of those data which are considered to be labelled and unlabelled.\n",
        "labeled_dataset_to_be_trained_index = np.where(labeled_mask)[0]\n",
        "unlabeled_dataset_index = np.where(~labeled_mask)[0]\n",
        "\n",
        "\n",
        "# Get the actual data  by index which are considered to be labelled and unlabelled.\n",
        "labeled_dataset_to_be_trained_x = np.array([x_subset[index]  for index in labeled_dataset_to_be_trained_index])\n",
        "labeled_dataset_to_be_trained_y = np.array([y_subset[index]  for index in labeled_dataset_to_be_trained_index])\n",
        "\n",
        "unlabeled_dataset_x = np.array([x_subset[index]  for index in unlabeled_dataset_index])\n",
        "# unlabeled_dataset_y = np.array([y_subset[index]  for index in unlabeled_dataset_index])\n",
        "unique_labels = np.unique([y for y in labeled_dataset_to_be_trained_y])\n",
        "\n",
        "print('Shape of labeled dataset to be used for training inputs', labeled_dataset_to_be_trained_x.shape)\n",
        "print('Shape of labeled dataset to be used for training labels', labeled_dataset_to_be_trained_y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsNNtoSwzzp0",
        "outputId": "604e8c7b-d997-4c70-f21c-1eec66ca2cc6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of labeled dataset to be used for training inputs (100, 28, 28)\n",
            "Shape of labeled dataset to be used for training labels (100,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Plot plus splitting datasets\n",
        "\n",
        "# Display one random image from x_train_subset\n",
        "index = np.random.randint(0, len(labeled_dataset_to_be_trained_x))\n",
        "image = labeled_dataset_to_be_trained_x[index]\n",
        "\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.title(f\"Label: {labeled_dataset_to_be_trained_y[index]}\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# train_labels_one_hot_y = keras.utils.to_categorical(unique_labels, num_classes=10)\n",
        "\n",
        "# Split the subset into training and testing sets\n",
        "labeled_x_train_subset,labeled_x_test_subset, labeled_y_train_subset, labeled_y_test_subset = train_test_split(\n",
        "    labeled_dataset_to_be_trained_x, labeled_dataset_to_be_trained_y, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "train_labels_y = keras.utils.to_categorical(labeled_y_train_subset, num_classes=10)\n",
        "test_labels_y = keras.utils.to_categorical(labeled_y_test_subset, num_classes=10)\n",
        "\n",
        "\n",
        "print('Shape of training data labels in one hot encode vector after splitting in 10% ', train_labels_y.shape)\n",
        "print('Shape of training data inputs after splitting in 10% ', train_labels_y.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "RFNaPRZMJ5am",
        "outputId": "b112d5cb-a541-4504-ae63-03106973fce0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of training data labels in one hot encode vector after splitting in 10%  (90, 10)\n",
            "Shape of training data inputs after splitting in 10%  (90, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the neural network model\n",
        "model = keras.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28,1)),  # Assuming input images are 28x28\n",
        "    layers.Dense(200, activation='relu'),   # Hidden layer with 200 neurons and ReLU activation\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "# model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "# model.fit(labeled_x_train_subset, train_labels_y , epochs=20, batch_size=64, validation_data=(labeled_x_test_subset, test_labels_y))\n",
        "model.fit(labeled_x_train_subset, train_labels_y , epochs=100, batch_size=20)\n",
        "\n",
        "print(labeled_x_test_subset.shape)\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(labeled_x_test_subset, test_labels_y)\n",
        "test_loss_ul, test_acc_ul = model.evaluate(unlabeled_dataset_x)\n",
        "\n",
        "print('\\nTest accuracy:', test_acc)\n",
        "print('\\nTest accuracy for unlabeled data set:', test_acc_ul)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zMQPODd5fYK",
        "outputId": "10da7b0c-8b3b-4553-d3b7-06a329620f9e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "5/5 [==============================] - 1s 7ms/step - loss: 194.0900 - accuracy: 0.1222\n",
            "Epoch 2/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 101.0935 - accuracy: 0.2889\n",
            "Epoch 3/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 66.6839 - accuracy: 0.3667\n",
            "Epoch 4/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 43.6076 - accuracy: 0.5556\n",
            "Epoch 5/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 21.7253 - accuracy: 0.6556\n",
            "Epoch 6/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 16.7318 - accuracy: 0.7222\n",
            "Epoch 7/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 6.7951 - accuracy: 0.8556\n",
            "Epoch 8/100\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 4.2669 - accuracy: 0.8778\n",
            "Epoch 9/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 5.7025 - accuracy: 0.8889\n",
            "Epoch 10/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 4.2592 - accuracy: 0.9000\n",
            "Epoch 11/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 3.9195 - accuracy: 0.8889\n",
            "Epoch 12/100\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 3.2197 - accuracy: 0.8778\n",
            "Epoch 13/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 3.8282 - accuracy: 0.9000\n",
            "Epoch 14/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 3.9205 - accuracy: 0.9111\n",
            "Epoch 15/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 4.1212 - accuracy: 0.9111\n",
            "Epoch 16/100\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 4.1819 - accuracy: 0.9111\n",
            "Epoch 17/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.8848 - accuracy: 0.9556\n",
            "Epoch 18/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.8508 - accuracy: 0.9333\n",
            "Epoch 19/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 1.8061 - accuracy: 0.9444\n",
            "Epoch 20/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.4476 - accuracy: 0.9556\n",
            "Epoch 21/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 1.4639 - accuracy: 0.9556\n",
            "Epoch 22/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 1.2555 - accuracy: 0.9556\n",
            "Epoch 23/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 3.3882 - accuracy: 0.9222\n",
            "Epoch 24/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 1.0098 - accuracy: 0.9667\n",
            "Epoch 25/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.9480 - accuracy: 0.9556\n",
            "Epoch 26/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 1.3108 - accuracy: 0.9333\n",
            "Epoch 27/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.5864 - accuracy: 0.9556\n",
            "Epoch 28/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.0249 - accuracy: 0.9889\n",
            "Epoch 29/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 2.1336 - accuracy: 0.9222\n",
            "Epoch 30/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.8830 - accuracy: 0.9556\n",
            "Epoch 31/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 3.7804e-04 - accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 2.5001 - accuracy: 0.9111\n",
            "Epoch 33/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.2762 - accuracy: 0.9778\n",
            "Epoch 34/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.5962 - accuracy: 0.9778\n",
            "Epoch 35/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.7970 - accuracy: 0.9778\n",
            "Epoch 36/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 1.4330 - accuracy: 0.9556\n",
            "Epoch 37/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 1.3703 - accuracy: 0.9222\n",
            "Epoch 38/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3645 - accuracy: 0.9667\n",
            "Epoch 39/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.0320 - accuracy: 0.9889\n",
            "Epoch 40/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 2.6025 - accuracy: 0.9444\n",
            "Epoch 41/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.5094 - accuracy: 0.9556\n",
            "Epoch 42/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.2805 - accuracy: 0.9667\n",
            "Epoch 43/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.7004 - accuracy: 0.9778\n",
            "Epoch 44/100\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.9190 - accuracy: 0.9667\n",
            "Epoch 45/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1036 - accuracy: 0.9778\n",
            "Epoch 46/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.7131 - accuracy: 0.9889\n",
            "Epoch 47/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 2.4645 - accuracy: 0.9333\n",
            "Epoch 48/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.3331 - accuracy: 0.9889\n",
            "Epoch 49/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.4696 - accuracy: 0.9667\n",
            "Epoch 50/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.2786 - accuracy: 0.9778\n",
            "Epoch 51/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.3741 - accuracy: 0.9889\n",
            "Epoch 52/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.2832 - accuracy: 0.9667\n",
            "Epoch 53/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.3675 - accuracy: 0.9667\n",
            "Epoch 54/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 3.0683e-05 - accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.0645 - accuracy: 0.9889\n",
            "Epoch 56/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.4241 - accuracy: 0.9667\n",
            "Epoch 57/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.2282 - accuracy: 0.9778\n",
            "Epoch 58/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 5.9112e-06 - accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.4772 - accuracy: 0.9889\n",
            "Epoch 60/100\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 2.3143e-04 - accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1967 - accuracy: 0.9889\n",
            "Epoch 62/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.6774 - accuracy: 0.9556\n",
            "Epoch 63/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 2.2106 - accuracy: 0.9444\n",
            "Epoch 64/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.3644 - accuracy: 0.9667\n",
            "Epoch 65/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.0370 - accuracy: 0.9889\n",
            "Epoch 66/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 1.0597 - accuracy: 0.9778\n",
            "Epoch 67/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.2271 - accuracy: 0.9667\n",
            "Epoch 68/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.2392 - accuracy: 0.9889\n",
            "Epoch 69/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.8378 - accuracy: 0.9667\n",
            "Epoch 70/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.2610 - accuracy: 0.9889\n",
            "Epoch 71/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 1.4073e-04 - accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.0886 - accuracy: 0.9889\n",
            "Epoch 73/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.5013 - accuracy: 0.9778\n",
            "Epoch 74/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 1.0002 - accuracy: 0.9778\n",
            "Epoch 75/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.5384 - accuracy: 0.9778\n",
            "Epoch 76/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.7107 - accuracy: 0.9778\n",
            "Epoch 77/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.2438 - accuracy: 0.9667\n",
            "Epoch 78/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.4104 - accuracy: 0.9889\n",
            "Epoch 79/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.0545 - accuracy: 0.9889\n",
            "Epoch 80/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.8076 - accuracy: 0.9667\n",
            "Epoch 81/100\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.3509 - accuracy: 0.9889\n",
            "Epoch 82/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.0956 - accuracy: 0.9778\n",
            "Epoch 83/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 1.5467 - accuracy: 0.9444\n",
            "Epoch 84/100\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 5.9466e-05 - accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 1.7032e-05 - accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 2.1520 - accuracy: 0.9556\n",
            "Epoch 87/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.6106 - accuracy: 0.9667\n",
            "Epoch 88/100\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.9667 - accuracy: 0.9667\n",
            "Epoch 89/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.0910 - accuracy: 0.9889\n",
            "Epoch 90/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.2785 - accuracy: 0.9667\n",
            "Epoch 91/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1876 - accuracy: 0.9778\n",
            "Epoch 92/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.6914 - accuracy: 0.9778\n",
            "Epoch 93/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.4968 - accuracy: 0.9778\n",
            "Epoch 94/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.6358 - accuracy: 0.9667\n",
            "Epoch 95/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 1.3121 - accuracy: 0.9444\n",
            "Epoch 96/100\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 2.4568e-04 - accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 1.1943 - accuracy: 0.9778\n",
            "Epoch 98/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1150 - accuracy: 0.9889\n",
            "Epoch 99/100\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.4033 - accuracy: 0.9889\n",
            "Epoch 100/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.2362 - accuracy: 0.9778\n",
            "(10, 28, 28)\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 27.1534 - accuracy: 0.6000\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00\n",
            "\n",
            "Test accuracy: 0.6000000238418579\n",
            "\n",
            "Test accuracy for unlabeled data set: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Virtual Adversarial Training\n",
        "x = labeled_x_train_subset\n",
        "\n",
        "def generate_random_unit_vector(input):\n",
        "    x = np.random.normal(0, 1, input.shape)\n",
        "    d = x / np.linalg.norm(x)\n",
        "    return d\n",
        "\n",
        "\n",
        "r = tf.random.normal(shape=tf.shape(labeled_x_train_subset))\n",
        "# r =  generate_random_unit_vector(r)\n",
        "perturbed_input = labeled_x_train_subset+0.01*r\n",
        "\n",
        "model_vat = keras.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28,1)),  # Assuming input images are 28x28\n",
        "    layers.Dense(200, activation='relu'),   # Hidden layer with 200 neurons and ReLU activation\n",
        "    layers.Dense(10, activation='softmax')\n",
        "\n",
        "])\n",
        "\n",
        "alpha = 0.001  # A hyperparameter for controlling the strength of the perturbation\n",
        "optimizer = keras.optimizers.Adam()\n",
        "model_vat.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "def virtual_adversarial_loss(x, logits):\n",
        "    d = generate_random_unit_vector(x)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(x)\n",
        "        logits_perturbed = model_vat(x + alpha * d)\n",
        "        loss = tf.keras.losses.categorical_crossentropy(logits, logits_perturbed)\n",
        "\n",
        "    r_vadv = tape.gradient(loss, x)\n",
        "    r_vadv_normalized = alpha * r_vadv / tf.norm(r_vadv)\n",
        "\n",
        "    logits_perturbed = model_vat(x + r_vadv_normalized)\n",
        "    loss = tf.keras.losses.categorical_crossentropy(logits, logits_perturbed)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i in range(0, len(perturbed_input), batch_size):\n",
        "        x_batch = perturbed_input[i:i+batch_size]\n",
        "        y_batch = train_labels_y[i:i+batch_size]\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model_vat(x_batch)\n",
        "            classification_loss = tf.keras.losses.categorical_crossentropy(y_batch, logits)\n",
        "            vat_loss = virtual_adversarial_loss(x_batch, logits)\n",
        "            total_loss = classification_loss + vat_loss\n",
        "        gradients = tape.gradient(total_loss, model_vat.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model_vat.trainable_variables))\n",
        "\n",
        "    if i % 40 == 0:\n",
        "              mean_total_loss = tf.reduce_mean(total_loss).numpy()\n",
        "              # print(mean_total_loss, )\n",
        "              print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/80] Total Loss: {mean_total_loss:.4f}')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model_vat.evaluate(labeled_x_test_subset, test_labels_y)\n",
        "print(\"Test accuracy for VAT:\", test_acc)\n",
        "# Evaluate the model on the unlabeled data set, first use mode to predict the logit\n",
        "\n",
        "unlabeled_predictions = model_vat.predict(unlabeled_dataset_x)\n",
        "unlabeled_predictions_exact = np.argmax(unlabeled_predictions, axis=1)\n",
        "unlabeled_dataset_y_for_vat = np.array([y_subset[index]  for index in unlabeled_dataset_index])\n",
        "\n",
        "print(unlabeled_predictions_exact.shape, 'shape', unlabeled_dataset_y_for_vat.shape)\n",
        "accuracy_unlabeled = np.mean(unlabeled_predictions_exact == unlabeled_dataset_y_for_vat)\n",
        "print(\"Accuracy on unlabeled dataset:\", accuracy_unlabeled)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVKj5jlQANhP",
        "outputId": "aa3ccefa-32e5-4dd6-955e-8f7f2caa5d88"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Step [81/80] Total Loss: 75.1646\n",
            "Epoch [2/20], Step [81/80] Total Loss: 61.8200\n",
            "Epoch [3/20], Step [81/80] Total Loss: 8.7786\n",
            "Epoch [4/20], Step [81/80] Total Loss: 6.0568\n",
            "Epoch [5/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [6/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [7/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [8/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [9/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [10/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [11/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [12/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [13/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [14/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [15/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [16/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [17/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [18/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [19/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [20/20], Step [81/80] Total Loss: 0.0000\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 32.8097 - accuracy: 0.5000\n",
            "Test accuracy for VAT: 0.5\n",
            "310/310 [==============================] - 1s 2ms/step\n",
            "(9900,) shape (9900,)\n",
            "Accuracy on unlabeled dataset: 0.6225252525252525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entropy minimization\n",
        "x = labeled_x_train_subset\n",
        "\n",
        "model_entropy_minimization = keras.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28,1)),  # Assuming input images are 28x28\n",
        "    layers.Dense(200, activation='relu'),   # Hidden layer with 200 neurons and ReLU activation\n",
        "    layers.Dense(10, activation='softmax')\n",
        "\n",
        "])\n",
        "\n",
        "alpha = 0.001  # A hyperparameter for controlling the strength of the perturbation\n",
        "optimizer = keras.optimizers.Adam()\n",
        "model_entropy_minimization.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "def entropy_minimization_loss(y_true, y_pred, alpha=0.001):\n",
        "    # Calculating the standard cross-entropy loss\n",
        "    # print(\"INSIDE ENTROPY MINIMIZATION LOSS\", y_true, y_pred)\n",
        "    cross_entropy_loss = categorical_crossentropy(y_true, y_pred)\n",
        "    # print(cross_entropy_loss.shape)\n",
        "\n",
        "\n",
        "    # Calculating the entropy of the predicted probabilities\n",
        "    epsilon = 1e-10\n",
        "    entropy = tf.reduce_sum(-y_pred * tf.math.log(y_pred + epsilon), axis=1)\n",
        "    # print('afdkafjk', entropy)\n",
        "\n",
        "    # Combining the cross-entropy loss with the entropy regularization term\n",
        "    total_loss = cross_entropy_loss + alpha * entropy\n",
        "\n",
        "    # tf.debugging.check_numerics(cross_entropy_loss, \"cross_entropy_loss is NaN or Inf\")\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i in range(0, len(x), batch_size):\n",
        "        x_batch = x[i:i+batch_size]\n",
        "        y_batch = train_labels_y[i:i+batch_size]\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model_entropy_minimization(x_batch)\n",
        "            total_loss = entropy_minimization_loss(y_batch,logits )\n",
        "        gradients = tape.gradient(total_loss, model_entropy_minimization.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model_entropy_minimization.trainable_variables))\n",
        "\n",
        "    if i % 40 == 0:\n",
        "              mean_total_loss = tf.reduce_mean(total_loss).numpy()\n",
        "              # print(mean_total_loss, )\n",
        "              print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/80] Total Loss: {mean_total_loss:.4f}')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model_entropy_minimization.evaluate(labeled_x_test_subset, test_labels_y)\n",
        "print(\"Test accuracy for Entropy minimization:\", test_acc)\n",
        "# Evaluate the model on the unlabeled data set, first use mode to predict the logit\n",
        "\n",
        "unlabeled_predictions = model_entropy_minimization.predict(unlabeled_dataset_x)\n",
        "unlabeled_predictions_exact = np.argmax(unlabeled_predictions, axis=1)\n",
        "unlabeled_dataset_y_for_entropy_minimization = np.array([y_subset[index]  for index in unlabeled_dataset_index])\n",
        "\n",
        "print(unlabeled_predictions_exact.shape, 'shape', unlabeled_dataset_y_for_entropy_minimization.shape)\n",
        "accuracy_unlabeled = np.mean(unlabeled_predictions_exact == unlabeled_dataset_y_for_entropy_minimization)\n",
        "print(\"Accuracy on unlabeled dataset for entropy minimization:\", accuracy_unlabeled)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umh2yWQzE7oy",
        "outputId": "343b48d2-89a4-4b5d-c11c-d5f77cbff534"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Step [81/80] Total Loss: 62.3311\n",
            "Epoch [2/20], Step [81/80] Total Loss: 26.9614\n",
            "Epoch [3/20], Step [81/80] Total Loss: 4.2976\n",
            "Epoch [4/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [5/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [6/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [7/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [8/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [9/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [10/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [11/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [12/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [13/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [14/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [15/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [16/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [17/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [18/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [19/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [20/20], Step [81/80] Total Loss: 0.0000\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 58.8582 - accuracy: 0.2000\n",
            "Test accuracy for Entropy minimization: 0.20000000298023224\n",
            "310/310 [==============================] - 1s 2ms/step\n",
            "(9900,) shape (9900,)\n",
            "Accuracy on unlabeled dataset for entropy minimization: 0.6129292929292929\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pseudo labels:\n",
        "pseudo_label_predictions = model.predict(unlabeled_dataset_x)\n",
        "max_predictions_value = np.argmax(pseudo_label_predictions)\n",
        "\n",
        "\n",
        "\n",
        "train_labels_y_value = keras.utils.to_categorical(labeled_dataset_to_be_trained_y, num_classes=10)\n",
        "#  combine labeled & unlabeled dataset:\n",
        "new_train_x = np.vstack((labeled_dataset_to_be_trained_x, unlabeled_dataset_x))\n",
        "new_train_y = np.vstack((train_labels_y_value, pseudo_label_predictions))\n",
        "\n",
        "\n",
        "print(labeled_dataset_to_be_trained_x.shape, unlabeled_dataset_x.shape, new_train_x.shape)\n",
        "print(train_labels_y.shape, pseudo_label_predictions.shape,new_train_y.shape)\n",
        "\n",
        "# train the model with this dataset\n",
        "labeled_x_train_subset_pseudo,labeled_x_test_subset_pseudo, labeled_y_train_subset_pseudo, labeled_y_test_subset_pseudo = train_test_split(\n",
        "    new_train_x, new_train_y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# train_labels_y_pseudo = keras.utils.to_categorical(labeled_y_train_subset_pseudo, num_classes=10)\n",
        "# test_labels_y_pseudo = keras.utils.to_categorical(labeled_y_test_subset_pseudo, num_classes=10)\n",
        "\n",
        "# Extract true labels for the labeled subset\n",
        "train_labels_y_pseudo = labeled_y_train_subset_pseudo[:, :10]\n",
        "test_labels_y_pseudo = labeled_y_test_subset_pseudo[:, :10]\n",
        "\n",
        "print(train_labels_y_pseudo.shape, test_labels_y_pseudo.shape, labeled_x_train_subset_pseudo.shape)\n",
        "# Train the model\n",
        "model.fit(labeled_x_train_subset_pseudo, train_labels_y_pseudo , epochs=20, batch_size=64)\n",
        "# model.fit(labeled_x_train_subset_pseudo, train_labels_y_pseudo , epochs=20, batch_size=64, validation_data=(labeled_x_test_subset_pseudo, test_labels_y_pseudo))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(labeled_x_test_subset_pseudo, test_labels_y_pseudo)\n",
        "print('\\nTest accuracy:', test_acc)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f06GRcr_LYA",
        "outputId": "4ba1fe9f-3757-4f51-ef5c-d7c7422c046f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "310/310 [==============================] - 1s 2ms/step\n",
            "(100, 28, 28) (9900, 28, 28) (10000, 28, 28)\n",
            "(90, 10) (9900, 10) (10000, 10)\n",
            "(8000, 10) (2000, 10) (8000, 28, 28)\n",
            "Epoch 1/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 5.5136 - accuracy: 0.7164\n",
            "Epoch 2/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 1.8345 - accuracy: 0.6413\n",
            "Epoch 3/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 1.3382 - accuracy: 0.6539\n",
            "Epoch 4/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 1.1739 - accuracy: 0.6687\n",
            "Epoch 5/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 1.0465 - accuracy: 0.6954\n",
            "Epoch 6/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.9752 - accuracy: 0.6955\n",
            "Epoch 7/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.9734 - accuracy: 0.7181\n",
            "Epoch 8/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.8995 - accuracy: 0.7230\n",
            "Epoch 9/20\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.8353 - accuracy: 0.7393\n",
            "Epoch 10/20\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.8428 - accuracy: 0.7321\n",
            "Epoch 11/20\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.8045 - accuracy: 0.7487\n",
            "Epoch 12/20\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.7901 - accuracy: 0.7581\n",
            "Epoch 13/20\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.7579 - accuracy: 0.7559\n",
            "Epoch 14/20\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.7498 - accuracy: 0.7588\n",
            "Epoch 15/20\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.7555 - accuracy: 0.7629\n",
            "Epoch 16/20\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.7495 - accuracy: 0.7695\n",
            "Epoch 17/20\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.7561 - accuracy: 0.7631\n",
            "Epoch 18/20\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.7443 - accuracy: 0.7673\n",
            "Epoch 19/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.7516 - accuracy: 0.7648\n",
            "Epoch 20/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.7387 - accuracy: 0.7670\n",
            "63/63 [==============================] - 0s 2ms/step - loss: 0.9358 - accuracy: 0.8000\n",
            "\n",
            "Test accuracy: 0.800000011920929\n"
          ]
        }
      ]
    }
  ]
}