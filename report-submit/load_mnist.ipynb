{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.losses import KLDivergence, sparse_categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "tf.random.set_seed(42)\n"
      ],
      "metadata": {
        "id": "xGqzAgZwzE5S"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "ZEpiD4KQzAVW",
        "outputId": "828d392a-e00a-499c-ffd3-fb6dae24631a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Combine training and test datasets to get the full dataset\n",
        "x_full = np.concatenate((x_train, x_test), axis=0)\n",
        "y_full = np.concatenate((y_train, y_test), axis=0)\n",
        "\n",
        "\n",
        "# Function to select a specified number of samples for each category\n",
        "def select_samples(x, y, num_samples):\n",
        "    selected_samples = []\n",
        "    selected_labels = []\n",
        "    for i in range(10):  # 10 categories in MNIST\n",
        "        indices = np.where(y == i)[0][:num_samples]\n",
        "        selected_samples.append(x[indices])\n",
        "        selected_labels.append(y[indices])\n",
        "    selected_samples = np.concatenate(selected_samples, axis=0)\n",
        "    selected_labels = np.concatenate(selected_labels, axis=0)\n",
        "    return selected_samples, selected_labels\n",
        "\n",
        "# Select 1,000 samples for each category\n",
        "x_subset, y_subset = select_samples(x_full, y_full, 1000)\n",
        "\n",
        "# Display one random image from x_train_subset\n",
        "index = np.random.randint(0, len(x_subset))\n",
        "image = x_subset[index]\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.title(f\"Label: {y_subset[index]}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing data set i.e 100 labeled datas i.e 10 labeled from each category.\n",
        "np.random.seed(0)\n",
        "labeled_mask = np.zeros(10000, dtype=bool)\n",
        "for i in range(10):\n",
        "    indices = np.where(np.array(y_subset) == i)[0] # returns shuffled array of indices of each class, total 1000 data.\n",
        "    np.random.shuffle(indices)\n",
        "    labeled_mask[indices[:10]] = True\n",
        "\n",
        "\n",
        "# Get the index of those data which are considered to be labelled and unlabelled.\n",
        "labeled_dataset_to_be_trained_index = np.where(labeled_mask)[0]\n",
        "unlabeled_dataset_index = np.where(~labeled_mask)[0]\n",
        "\n",
        "\n",
        "# Get the actual data  by index which are considered to be labelled and unlabelled.\n",
        "labeled_dataset_to_be_trained_x = np.array([x_subset[index]  for index in labeled_dataset_to_be_trained_index])\n",
        "labeled_dataset_to_be_trained_y = np.array([y_subset[index]  for index in labeled_dataset_to_be_trained_index])\n",
        "\n",
        "unlabeled_dataset_x = np.array([x_subset[index]  for index in unlabeled_dataset_index])\n",
        "# unlabeled_dataset_y = np.array([y_subset[index]  for index in unlabeled_dataset_index])\n",
        "unique_labels = np.unique([y for y in labeled_dataset_to_be_trained_y])\n",
        "\n",
        "print('Shape of labeled dataset to be used for training inputs', labeled_dataset_to_be_trained_x.shape)\n",
        "print('Shape of labeled dataset to be used for training labels', labeled_dataset_to_be_trained_y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsNNtoSwzzp0",
        "outputId": "93e049e0-2272-463e-f84c-8870a5ab5ff7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of labeled dataset to be used for training inputs (100, 28, 28)\n",
            "Shape of labeled dataset to be used for training labels (100,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Plot plus splitting datasets\n",
        "\n",
        "# Display one random image from x_train_subset\n",
        "index = np.random.randint(0, len(labeled_dataset_to_be_trained_x))\n",
        "image = labeled_dataset_to_be_trained_x[index]\n",
        "\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.title(f\"Label: {labeled_dataset_to_be_trained_y[index]}\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# train_labels_one_hot_y = keras.utils.to_categorical(unique_labels, num_classes=10)\n",
        "\n",
        "# Split the subset into training and testing sets\n",
        "labeled_x_train_subset,labeled_x_test_subset, labeled_y_train_subset, labeled_y_test_subset = train_test_split(\n",
        "    labeled_dataset_to_be_trained_x, labeled_dataset_to_be_trained_y, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "train_labels_y = keras.utils.to_categorical(labeled_y_train_subset, num_classes=10)\n",
        "test_labels_y = keras.utils.to_categorical(labeled_y_test_subset, num_classes=10)\n",
        "\n",
        "\n",
        "print('Shape of training data labels in one hot encode vector after splitting in 10% ', train_labels_y.shape)\n",
        "print('Shape of training data inputs after splitting in 10% ', train_labels_y.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "RFNaPRZMJ5am",
        "outputId": "cfab1a6c-7f71-4ee0-a261-a13506d07efc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of training data labels in one hot encode vector after splitting in 10%  (90, 10)\n",
            "Shape of training data inputs after splitting in 10%  (90, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the neural network model\n",
        "model = keras.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28,1)),  # Assuming input images are 28x28\n",
        "    layers.Dense(200, activation='relu'),   # Hidden layer with 200 neurons and ReLU activation\n",
        "    # layers.Dropout(0.5),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "# model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "# model.fit(labeled_x_train_subset, train_labels_y , epochs=20, batch_size=64, validation_data=(labeled_x_test_subset, test_labels_y))\n",
        "model.fit(labeled_x_train_subset, train_labels_y , epochs=20, batch_size=10)\n",
        "\n",
        "\n",
        "train_loss, train_accuracy = model.evaluate(labeled_x_train_subset, train_labels_y, verbose=0)\n",
        "print(f'Train Accuracy: {train_accuracy:.4f}')\n",
        "\n",
        "# Evaluate on the test set\n",
        "test_loss, test_accuracy = model.evaluate(labeled_x_test_subset, test_labels_y, verbose=0)\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "\n",
        "# Evaluate the model on the unlabeled dataset and\n",
        "test_loss_ul, test_acc_ul = model.evaluate(unlabeled_dataset_x)\n",
        "print('\\nTest accuracy for unlabeled data set:', test_acc_ul)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zMQPODd5fYK",
        "outputId": "7f10fccc-dbe2-425a-817f-3baa82034023"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "9/9 [==============================] - 1s 5ms/step - loss: 104.4908 - accuracy: 0.2333\n",
            "Epoch 2/20\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 25.8098 - accuracy: 0.6333\n",
            "Epoch 3/20\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 5.6223 - accuracy: 0.8333\n",
            "Epoch 4/20\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.3643 - accuracy: 0.9667\n",
            "Epoch 5/20\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.3681 - accuracy: 0.9778\n",
            "Epoch 6/20\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.1209 - accuracy: 0.9889\n",
            "Epoch 7/20\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.4290 - accuracy: 0.9889\n",
            "Epoch 8/20\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 6.6227e-09 - accuracy: 1.0000\n",
            "Epoch 9/20\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 3.6682e-05 - accuracy: 1.0000\n",
            "Epoch 10/20\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.2179 - accuracy: 0.9889\n",
            "Epoch 11/20\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 12/20\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0053 - accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Train Accuracy: 1.0000\n",
            "Test Accuracy: 0.6000\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00\n",
            "\n",
            "Test accuracy for unlabeled data set: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Virtual Adversarial Training\n",
        "x = labeled_x_train_subset\n",
        "\n",
        "def generate_random_unit_vector(input):\n",
        "    x = np.random.normal(0, 1, input.shape)\n",
        "    d = x / np.linalg.norm(x)\n",
        "    return d\n",
        "\n",
        "\n",
        "r = tf.random.normal(shape=tf.shape(labeled_x_train_subset))\n",
        "# r =  generate_random_unit_vector(r)\n",
        "perturbed_input = labeled_x_train_subset+0.01*r\n",
        "\n",
        "model_vat = keras.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28,1)),  # Assuming input images are 28x28\n",
        "    layers.Dense(200, activation='relu'),   # Hidden layer with 200 neurons and ReLU activation\n",
        "     layers.Dropout(0.5),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "\n",
        "])\n",
        "\n",
        "alpha = 0.001  # A hyperparameter for controlling the strength of the perturbation\n",
        "optimizer = keras.optimizers.Adam()\n",
        "model_vat.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "def virtual_adversarial_loss(x, logits):\n",
        "    d = generate_random_unit_vector(x)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(x)\n",
        "        logits_perturbed = model_vat(x + alpha * d)\n",
        "        loss = tf.keras.losses.categorical_crossentropy(logits, logits_perturbed)\n",
        "\n",
        "    r_vadv = tape.gradient(loss, x)\n",
        "    r_vadv_normalized = alpha * r_vadv / tf.norm(r_vadv)\n",
        "\n",
        "    logits_perturbed = model_vat(x + r_vadv_normalized)\n",
        "    loss = tf.keras.losses.categorical_crossentropy(logits, logits_perturbed)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i in range(0, len(perturbed_input), batch_size):\n",
        "        x_batch = perturbed_input[i:i+batch_size]\n",
        "        y_batch = train_labels_y[i:i+batch_size]\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model_vat(x_batch)\n",
        "            classification_loss = tf.keras.losses.categorical_crossentropy(y_batch, logits)\n",
        "            vat_loss = virtual_adversarial_loss(x_batch, logits)\n",
        "            total_loss = classification_loss + vat_loss\n",
        "        gradients = tape.gradient(total_loss, model_vat.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model_vat.trainable_variables))\n",
        "\n",
        "    if i % 40 == 0:\n",
        "              mean_total_loss = tf.reduce_mean(total_loss).numpy()\n",
        "              # print(mean_total_loss, )\n",
        "              print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/40] Total Loss: {mean_total_loss:.4f}')\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate the model on the train set\n",
        "train_loss, train_accuracy = model_vat.evaluate(labeled_x_train_subset, train_labels_y, verbose=0)\n",
        "print(f'Train Accuracy: {train_accuracy:.4f}')\n",
        "\n",
        "# Evaluate on the test set\n",
        "test_loss, test_accuracy = model_vat.evaluate(labeled_x_test_subset, test_labels_y, verbose=0)\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "# Evaluate the model on the unlabeled data set, first use mode to predict the logit\n",
        "unlabeled_predictions = model_vat.predict(unlabeled_dataset_x)\n",
        "unlabeled_predictions_exact = np.argmax(unlabeled_predictions, axis=1)\n",
        "unlabeled_dataset_y_for_vat = np.array([y_subset[index]  for index in unlabeled_dataset_index])\n",
        "\n",
        "accuracy_unlabeled = np.mean(unlabeled_predictions_exact == unlabeled_dataset_y_for_vat)\n",
        "print(\"Accuracy on unlabeled dataset:\", accuracy_unlabeled)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVKj5jlQANhP",
        "outputId": "322d3afd-f79b-4e1e-f002-c0630831547d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Step [81/40] Total Loss: 107.8862\n",
            "Epoch [2/20], Step [81/40] Total Loss: 27.0164\n",
            "Epoch [3/20], Step [81/40] Total Loss: 0.0164\n",
            "Epoch [4/20], Step [81/40] Total Loss: 2.2050\n",
            "Epoch [5/20], Step [81/40] Total Loss: 0.0000\n",
            "Epoch [6/20], Step [81/40] Total Loss: 0.0000\n",
            "Epoch [7/20], Step [81/40] Total Loss: 0.0000\n",
            "Epoch [8/20], Step [81/40] Total Loss: 0.0000\n",
            "Epoch [9/20], Step [81/40] Total Loss: 0.0000\n",
            "Epoch [10/20], Step [81/40] Total Loss: 0.0000\n",
            "Epoch [11/20], Step [81/40] Total Loss: 0.0000\n",
            "Epoch [12/20], Step [81/40] Total Loss: 0.0001\n",
            "Epoch [13/20], Step [81/40] Total Loss: 0.0020\n",
            "Epoch [14/20], Step [81/40] Total Loss: 0.0000\n",
            "Epoch [15/20], Step [81/40] Total Loss: 0.0000\n",
            "Epoch [16/20], Step [81/40] Total Loss: 0.0000\n",
            "Epoch [17/20], Step [81/40] Total Loss: 0.0000\n",
            "Epoch [18/20], Step [81/40] Total Loss: 0.0000\n",
            "Epoch [19/20], Step [81/40] Total Loss: 0.0000\n",
            "Epoch [20/20], Step [81/40] Total Loss: 0.0000\n",
            "Train Accuracy: 1.0000\n",
            "Test Accuracy: 0.5000\n",
            "310/310 [==============================] - 1s 2ms/step\n",
            "Accuracy on unlabeled dataset: 0.6322222222222222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entropy minimization\n",
        "x = labeled_x_train_subset\n",
        "\n",
        "model_entropy_minimization = keras.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28,1)),  # Assuming input images are 28x28\n",
        "    layers.Dense(200, activation='relu'),   # Hidden layer with 200 neurons and ReLU activation\n",
        "    layers.Dense(10, activation='softmax')\n",
        "\n",
        "])\n",
        "\n",
        "alpha = 0.001  # A hyperparameter for controlling the strength of the perturbation\n",
        "optimizer = keras.optimizers.Adam()\n",
        "model_entropy_minimization.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "def entropy_minimization_loss(y_true, y_pred, alpha=0.001):\n",
        "    # Calculating the standard cross-entropy loss\n",
        "    # print(\"INSIDE ENTROPY MINIMIZATION LOSS\", y_true, y_pred)\n",
        "    cross_entropy_loss = categorical_crossentropy(y_true, y_pred)\n",
        "    # print(cross_entropy_loss.shape)\n",
        "\n",
        "\n",
        "    # Calculating the entropy of the predicted probabilities\n",
        "    epsilon = 1e-10\n",
        "    entropy = tf.reduce_sum(-y_pred * tf.math.log(y_pred + epsilon), axis=1)\n",
        "    # print('afdkafjk', entropy)\n",
        "\n",
        "    # Combining the cross-entropy loss with the entropy regularization term\n",
        "    total_loss = cross_entropy_loss + alpha * entropy\n",
        "\n",
        "    # tf.debugging.check_numerics(cross_entropy_loss, \"cross_entropy_loss is NaN or Inf\")\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i in range(0, len(x), batch_size):\n",
        "        x_batch = x[i:i+batch_size]\n",
        "        y_batch = train_labels_y[i:i+batch_size]\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model_entropy_minimization(x_batch)\n",
        "            total_loss = entropy_minimization_loss(y_batch,logits )\n",
        "        gradients = tape.gradient(total_loss, model_entropy_minimization.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model_entropy_minimization.trainable_variables))\n",
        "\n",
        "    if i % 40 == 0:\n",
        "              mean_total_loss = tf.reduce_mean(total_loss).numpy()\n",
        "              # print(mean_total_loss, )\n",
        "              print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/40] Total Loss: {mean_total_loss:.4f}')\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate the model on the train set\n",
        "train_loss, train_accuracy = model_entropy_minimization.evaluate(labeled_x_train_subset, train_labels_y, verbose=0)\n",
        "print(f'Train Accuracy: {train_accuracy:.4f}')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model_entropy_minimization.evaluate(labeled_x_test_subset, test_labels_y)\n",
        "print(\"Test accuracy for Entropy minimization:\", test_acc)\n",
        "\n",
        "# Evaluate the model on the unlabeled data set, first use mode to predict the logit\n",
        "unlabeled_predictions = model_entropy_minimization.predict(unlabeled_dataset_x)\n",
        "unlabeled_predictions_exact = np.argmax(unlabeled_predictions, axis=1)\n",
        "unlabeled_dataset_y_for_entropy_minimization = np.array([y_subset[index]  for index in unlabeled_dataset_index])\n",
        "accuracy_unlabeled = np.mean(unlabeled_predictions_exact == unlabeled_dataset_y_for_entropy_minimization)\n",
        "print(\"Accuracy on unlabeled dataset for entropy minimization:\", accuracy_unlabeled)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umh2yWQzE7oy",
        "outputId": "b8d34ff2-0ff8-4a96-c95b-d5381825c678"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Step [81/40] Total Loss: 61.4233\n",
            "Epoch [2/20], Step [81/40] Total Loss: 27.8615\n",
            "Epoch [3/20], Step [81/40] Total Loss: 0.0000\n",
            "Epoch [4/20], Step [81/40] Total Loss: 0.0000\n",
            "Epoch [5/20], Step [81/40] Total Loss: 0.0000\n",
            "Epoch [6/20], Step [81/40] Total Loss: 0.0000\n",
            "Epoch [7/20], Step [81/40] Total Loss: 0.0000\n",
            "Epoch [8/20], Step [81/40] Total Loss: 0.0000\n",
            "Epoch [9/20], Step [81/40] Total Loss: 0.0013\n",
            "Epoch [10/20], Step [81/40] Total Loss: 0.0009\n",
            "Epoch [11/20], Step [81/40] Total Loss: 0.0000\n",
            "Epoch [12/20], Step [81/40] Total Loss: 0.0000\n",
            "Epoch [13/20], Step [81/40] Total Loss: 0.2104\n",
            "Epoch [14/20], Step [81/40] Total Loss: 0.0000\n",
            "Epoch [15/20], Step [81/40] Total Loss: 0.0000\n",
            "Epoch [16/20], Step [81/40] Total Loss: 0.0000\n",
            "Epoch [17/20], Step [81/40] Total Loss: 0.0000\n",
            "Epoch [18/20], Step [81/40] Total Loss: 0.0000\n",
            "Epoch [19/20], Step [81/40] Total Loss: 0.8075\n",
            "Epoch [20/20], Step [81/40] Total Loss: 0.0009\n",
            "Train Accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 42.5969 - accuracy: 0.4000\n",
            "Test accuracy for Entropy minimization: 0.4000000059604645\n",
            "310/310 [==============================] - 1s 2ms/step\n",
            "Accuracy on unlabeled dataset for entropy minimization: 0.6055555555555555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pseudo labels:\n",
        "pseudo_label_predictions = model.predict(unlabeled_dataset_x)\n",
        "max_predictions_value = np.argmax(pseudo_label_predictions)\n",
        "\n",
        "\n",
        "model_pseudo = keras.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28,1)),  # Assuming input images are 28x28\n",
        "    layers.Dense(200, activation='relu'),   # Hidden layer with 200 neurons and ReLU activation\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model_pseudo.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "train_labels_y_value = keras.utils.to_categorical(labeled_dataset_to_be_trained_y, num_classes=10)\n",
        "#  combine labeled & unlabeled dataset:\n",
        "new_train_x = np.vstack((labeled_dataset_to_be_trained_x, unlabeled_dataset_x))\n",
        "new_train_y = np.vstack((train_labels_y_value, pseudo_label_predictions))\n",
        "\n",
        "\n",
        "# print(labeled_dataset_to_be_trained_x.shape, unlabeled_dataset_x.shape, new_train_x.shape)\n",
        "# print(train_labels_y.shape, pseudo_label_predictions.shape,new_train_y.shape)\n",
        "\n",
        "# train the model with this dataset\n",
        "labeled_x_train_subset_pseudo,labeled_x_test_subset_pseudo, labeled_y_train_subset_pseudo, labeled_y_test_subset_pseudo = train_test_split(\n",
        "    new_train_x, new_train_y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# train_labels_y_pseudo = keras.utils.to_categorical(labeled_y_train_subset_pseudo, num_classes=10)\n",
        "# test_labels_y_pseudo = keras.utils.to_categorical(labeled_y_test_subset_pseudo, num_classes=10)\n",
        "\n",
        "# Extract true labels for the labeled subset\n",
        "train_labels_y_pseudo = labeled_y_train_subset_pseudo[:, :10]\n",
        "test_labels_y_pseudo = labeled_y_test_subset_pseudo[:, :10]\n",
        "\n",
        "# print(train_labels_y_pseudo.shape, test_labels_y_pseudo.shape, labeled_x_train_subset_pseudo.shape)\n",
        "# Train the model\n",
        "model_pseudo.fit(labeled_x_train_subset_pseudo, train_labels_y_pseudo , epochs=20, batch_size=64)\n",
        "# model.fit(labeled_x_train_subset_pseudo, train_labels_y_pseudo , epochs=20, batch_size=64, validation_data=(labeled_x_test_subset_pseudo, test_labels_y_pseudo))\n",
        "\n",
        "\n",
        "# Evaluate the model on the train set\n",
        "train_loss, train_accuracy = model_pseudo.evaluate(labeled_x_train_subset_pseudo, train_labels_y_pseudo)\n",
        "print(f'Train Accuracy: {train_accuracy:.4f}')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model_pseudo.evaluate(labeled_x_test_subset_pseudo, test_labels_y_pseudo)\n",
        "print('\\nTest accuracy:', test_acc)\n",
        "\n",
        "# Evaluate the model on the unlabeled data set, first use model to predict the logit\n",
        "unlabeled_predictions = model_pseudo.predict(unlabeled_dataset_x)\n",
        "unlabeled_predictions_exact = np.argmax(unlabeled_predictions, axis=1)\n",
        "unlabeled_dataset_y_for_pseudo_labe = np.array([y_subset[index]  for index in unlabeled_dataset_index])\n",
        "accuracy_unlabeled = np.mean(unlabeled_predictions_exact == unlabeled_dataset_y_for_pseudo_labe)\n",
        "print(\"Accuracy on unlabeled dataset for pseudo labels:\", accuracy_unlabeled)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f06GRcr_LYA",
        "outputId": "f511f3c1-32a7-4b48-aa6e-be19c7858d82"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "310/310 [==============================] - 2s 6ms/step\n",
            "Epoch 1/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 17.7996 - accuracy: 0.6579\n",
            "Epoch 2/20\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 4.2496 - accuracy: 0.7601\n",
            "Epoch 3/20\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 2.2145 - accuracy: 0.7971\n",
            "Epoch 4/20\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 1.4498 - accuracy: 0.8319\n",
            "Epoch 5/20\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.9834 - accuracy: 0.8583\n",
            "Epoch 6/20\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.7540 - accuracy: 0.8751\n",
            "Epoch 7/20\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.6032 - accuracy: 0.8923\n",
            "Epoch 8/20\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.4598 - accuracy: 0.9101\n",
            "Epoch 9/20\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.3763 - accuracy: 0.9233\n",
            "Epoch 10/20\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.3421 - accuracy: 0.9269\n",
            "Epoch 11/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.3212 - accuracy: 0.9308\n",
            "Epoch 12/20\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.3343 - accuracy: 0.9305\n",
            "Epoch 13/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.3516 - accuracy: 0.9300\n",
            "Epoch 14/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.3307 - accuracy: 0.9301\n",
            "Epoch 15/20\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.2956 - accuracy: 0.9395\n",
            "Epoch 16/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.2538 - accuracy: 0.9431\n",
            "Epoch 17/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.2614 - accuracy: 0.9434\n",
            "Epoch 18/20\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.2597 - accuracy: 0.9454\n",
            "Epoch 19/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.3020 - accuracy: 0.9400\n",
            "Epoch 20/20\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.2931 - accuracy: 0.9402\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 0.2260 - accuracy: 0.9513\n",
            "Train Accuracy: 0.9513\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 2.4397 - accuracy: 0.7710\n",
            "\n",
            "Test accuracy: 0.7710000276565552\n",
            "310/310 [==============================] - 1s 2ms/step\n",
            "Accuracy on unlabeled dataset for pseudo labels: 0.6666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pseudo labels by self:\n",
        "\n",
        "model_pseudo = keras.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28,1)),  # Assuming input images are 28x28\n",
        "    layers.Dense(200, activation='relu'),   # Hidden layer with 200 neurons and ReLU activation\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model_pseudo.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "initial_label_predictions = np.zeros((9900,10))\n",
        "initial_label_predictions[:, 0] = 1\n",
        "\n",
        "train_labels_y_value = keras.utils.to_categorical(labeled_dataset_to_be_trained_y, num_classes=10)\n",
        "\n",
        "#  combine labeled & unlabeled dataset with initialization:\n",
        "new_train_x = np.vstack((labeled_dataset_to_be_trained_x, unlabeled_dataset_x))\n",
        "new_train_y = np.vstack((train_labels_y_value, initial_label_predictions))\n",
        "\n",
        "# train the model with this dataset\n",
        "labeled_x_train_subset_pseudo_self,labeled_x_test_subset_pseudo_self, labeled_y_train_subset_pseudo_self, labeled_y_test_subset_pseudo_self = train_test_split(\n",
        "    new_train_x, new_train_y, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "# Extract true labels for the labeled subset\n",
        "train_labels_y_pseudo = labeled_y_train_subset_pseudo_self[:, :10]\n",
        "test_labels_y_pseudo = labeled_y_test_subset_pseudo_self[:, :10]\n",
        "\n",
        "# Train the model\n",
        "model_pseudo.fit(labeled_x_train_subset_pseudo_self, train_labels_y_pseudo , epochs=20, batch_size=64)\n",
        "# model.fit(labeled_x_train_subset_pseudo_self, train_labels_y_pseudo , epochs=20, batch_size=64, validation_data=(labeled_x_test_subset_pseudo_self, test_labels_y_pseudo))\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate the model on the train set\n",
        "train_loss, train_accuracy = model_pseudo.evaluate(labeled_x_train_subset_pseudo_self, train_labels_y_pseudo)\n",
        "print(f'Train Accuracy: {train_accuracy:.4f}')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model_pseudo.evaluate(labeled_x_test_subset_pseudo_self, test_labels_y_pseudo)\n",
        "print('\\nTest accuracy for initialization:', test_acc)\n",
        "\n",
        "# Evaluate the model on the unlabeled data set, first use model to predict the logit\n",
        "unlabeled_predictions = model_pseudo.predict(unlabeled_dataset_x)\n",
        "unlabeled_predictions_exact = np.argmax(unlabeled_predictions, axis=1)\n",
        "unlabeled_dataset_y_for_pseudo_labe = np.array([y_subset[index]  for index in unlabeled_dataset_index])\n",
        "accuracy_unlabeled = np.mean(unlabeled_predictions_exact == unlabeled_dataset_y_for_pseudo_labe)\n",
        "print(\"Accuracy on unlabeled dataset for pseudo labels:\", accuracy_unlabeled)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqCfBvZ1MLJH",
        "outputId": "cfa46257-4675-425f-a29f-c60ceb740564"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "141/141 [==============================] - 1s 5ms/step - loss: 4.6028 - accuracy: 0.9792\n",
            "Epoch 2/20\n",
            "141/141 [==============================] - 1s 6ms/step - loss: 1.2891 - accuracy: 0.9820\n",
            "Epoch 3/20\n",
            "141/141 [==============================] - 1s 6ms/step - loss: 0.6359 - accuracy: 0.9846\n",
            "Epoch 4/20\n",
            "141/141 [==============================] - 1s 6ms/step - loss: 0.4035 - accuracy: 0.9878\n",
            "Epoch 5/20\n",
            "141/141 [==============================] - 1s 6ms/step - loss: 0.2678 - accuracy: 0.9884\n",
            "Epoch 6/20\n",
            "141/141 [==============================] - 1s 5ms/step - loss: 0.2641 - accuracy: 0.9887\n",
            "Epoch 7/20\n",
            "141/141 [==============================] - 1s 6ms/step - loss: 0.1311 - accuracy: 0.9922\n",
            "Epoch 8/20\n",
            "141/141 [==============================] - 1s 6ms/step - loss: 0.1749 - accuracy: 0.9921\n",
            "Epoch 9/20\n",
            "141/141 [==============================] - 1s 6ms/step - loss: 0.1284 - accuracy: 0.9937\n",
            "Epoch 10/20\n",
            "141/141 [==============================] - 1s 6ms/step - loss: 0.0947 - accuracy: 0.9939\n",
            "Epoch 11/20\n",
            "141/141 [==============================] - 1s 8ms/step - loss: 0.0884 - accuracy: 0.9940\n",
            "Epoch 12/20\n",
            "141/141 [==============================] - 1s 9ms/step - loss: 0.0821 - accuracy: 0.9948\n",
            "Epoch 13/20\n",
            "141/141 [==============================] - 1s 8ms/step - loss: 0.1607 - accuracy: 0.9931\n",
            "Epoch 14/20\n",
            "141/141 [==============================] - 1s 8ms/step - loss: 0.0696 - accuracy: 0.9954\n",
            "Epoch 15/20\n",
            "141/141 [==============================] - 1s 9ms/step - loss: 0.0726 - accuracy: 0.9953\n",
            "Epoch 16/20\n",
            "141/141 [==============================] - 1s 8ms/step - loss: 0.0689 - accuracy: 0.9958\n",
            "Epoch 17/20\n",
            "141/141 [==============================] - 1s 6ms/step - loss: 0.0582 - accuracy: 0.9956\n",
            "Epoch 18/20\n",
            "141/141 [==============================] - 1s 6ms/step - loss: 0.1246 - accuracy: 0.9932\n",
            "Epoch 19/20\n",
            "141/141 [==============================] - 1s 6ms/step - loss: 0.2065 - accuracy: 0.9912\n",
            "Epoch 20/20\n",
            "141/141 [==============================] - 1s 6ms/step - loss: 0.0939 - accuracy: 0.9942\n",
            "282/282 [==============================] - 1s 3ms/step - loss: 0.0704 - accuracy: 0.9969\n",
            "Train Accuracy: 0.9969\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1445 - accuracy: 0.9850\n",
            "\n",
            "Test accuracy for initialization: 0.9850000143051147\n",
            "310/310 [==============================] - 1s 3ms/step\n",
            "Accuracy on unlabeled dataset for pseudo labels: 0.10020202020202021\n"
          ]
        }
      ]
    }
  ]
}