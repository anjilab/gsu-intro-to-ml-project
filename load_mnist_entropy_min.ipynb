{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.losses import KLDivergence, sparse_categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n"
      ],
      "metadata": {
        "id": "xGqzAgZwzE5S"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "ZEpiD4KQzAVW",
        "outputId": "4a3e99d8-80ad-4846-8cf4-e747f2b1445f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGzCAYAAABpdMNsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAh8klEQVR4nO3de3BU9f3/8ddyWxCTxRBzU8AEUFQubVECVQElJaTVAcR6a0eoDhYarIK30paL2jZKtVI1XmaqpBTBCyNQnJZWA4FeCBQUGbwgiVGgkACx7EIwAZPP7w9+7tc1CXjCbt5JeD5mPjPZcz7vPe8cTvLi7J6c9TnnnAAAaGbtrBsAAJyeCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIOAUffzxx/L5fHr00Uej9pxFRUXy+XwqKiqK2nMCLQ0BhNNSQUGBfD6fNm3aZN1KzLz55pu68sorlZiYqG7dumnIkCH605/+ZN0WEEYAAW3Qn//8Z40ePVpHjx7V3Llz9etf/1pdunTRLbfcoscff9y6PUCS1MG6AQDR99RTTyk1NVWrV6+W3++XJP34xz9Wv379VFBQoOnTpxt3CHAGBDTq6NGjmj17tgYPHqxAIKCuXbvqiiuu0Jo1axqtefzxx9WrVy916dJFI0aM0LZt2+rN+eCDD3TdddcpISFBnTt31iWXXKI///nPJ+3nyJEj+uCDD3TgwIGTzg2FQjrrrLPC4SNJHTp0UGJiorp06XLSeqA5EEBAI0KhkP7whz9o5MiReuSRRzR37lzt379f2dnZ2rJlS735Cxcu1BNPPKHc3FzNnDlT27Zt01VXXaWKiorwnHfffVdDhw7V+++/r5/97Gd67LHH1LVrV40bN07Lli07YT8bN27UhRdeqKeeeuqkvY8cOVLvvvuuZs2apZKSEpWWluqhhx7Spk2bdN9993neF0BMOOA0tGDBAifJ/ec//2l0zueff+5qamoilv3vf/9zycnJ7tZbbw0vKysrc5Jcly5d3O7du8PLN2zY4CS56dOnh5eNGjXKDRgwwFVXV4eX1dXVuW9/+9uub9++4WVr1qxxktyaNWvqLZszZ85Jv7/Dhw+766+/3vl8PifJSXJnnHGGW758+UlrgebCGRDQiPbt26tTp06SpLq6On366af6/PPPdckll+itt96qN3/cuHE655xzwo+HDBmizMxM/eUvf5Ekffrpp1q9erWuv/56HTp0SAcOHNCBAwdUWVmp7Oxs7dixQ//9738b7WfkyJFyzmnu3Lkn7d3v9+v888/XddddpyVLlmjRokW65JJL9MMf/lDFxcUe9wQQG1yEAJzAH//4Rz322GP64IMPdOzYsfDy9PT0enP79u1bb9n555+vV155RZJUUlIi55xmzZqlWbNmNbi9ffv2RYRYU02bNk3FxcV666231K7d8f9nXn/99br44ot15513asOGDae8DeBUEUBAIxYtWqRJkyZp3Lhxuvfee5WUlKT27dsrLy9PpaWlnp+vrq5OknTPPfcoOzu7wTl9+vQ5pZ6l4xdPPP/887rvvvvC4SNJHTt2VE5Ojp566ikdPXo0fHYHWCGAgEYsXbpUGRkZeu211+Tz+cLL58yZ0+D8HTt21Fv24Ycf6rzzzpMkZWRkSDoeBFlZWdFv+P+rrKzU559/rtra2nrrjh07prq6ugbXAc2N94CARrRv316S5JwLL9uwYYPWr1/f4Pzly5dHvIezceNGbdiwQTk5OZKkpKQkjRw5Us8995z27t1br37//v0n7OfrXoadlJSkbt26admyZTp69Gh4+eHDh7Vy5Ur169ePS7HRInAGhNPaCy+8oFWrVtVbfuedd+rqq6/Wa6+9pvHjx+t73/ueysrK9Oyzz+qiiy7S4cOH69X06dNHl19+uaZOnaqamhrNnz9f3bt3j7jsOT8/X5dffrkGDBigyZMnKyMjQxUVFVq/fr12796td955p9FeN27cqCuvvFJz5sw54YUI7du31z333KNf/vKXGjp0qG655RbV1tbq+eef1+7du7Vo0SJvOwmIEQIIp7VnnnmmweWTJk3SpEmTVF5erueee05/+9vfdNFFF2nRokV69dVXG7xJ6C233KJ27dpp/vz52rdvn4YMGRK+I8EXLrroIm3atEkPPPCACgoKVFlZqaSkJH3zm9/U7Nmzo/Z9/eIXv1B6erp+//vf64EHHlBNTY0GDhyopUuXasKECVHbDnAqfO7Lry8AANBMeA8IAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJhocX8HVFdXpz179iguLi7i9icAgNbBOadDhw4pLS0t4n6EX9XiAmjPnj3q0aOHdRsAgFO0a9cunXvuuY2ub3EvwcXFxVm3AACIgpP9Po9ZAOXn5+u8885T586dlZmZqY0bN36tOl52A4C24WS/z2MSQC+//LJmzJihOXPm6K233tKgQYOUnZ2tffv2xWJzAIDWKBaf8z1kyBCXm5sbflxbW+vS0tJcXl7eSWuDwWD4M+wZDAaD0XpHMBg84e/7qJ8BHT16VJs3b474wK127dopKyurwc9RqampUSgUihgAgLYv6gF04MAB1dbWKjk5OWJ5cnKyysvL683Py8tTIBAID66AA4DTg/lVcDNnzlQwGAyPXbt2WbcEAGgGUf87oMTERLVv314VFRURyysqKpSSklJvvt/vl9/vj3YbAIAWLupnQJ06ddLgwYNVWFgYXlZXV6fCwkINGzYs2psDALRSMbkTwowZMzRx4kRdcsklGjJkiObPn6+qqir96Ec/isXmAACtUEwC6IYbbtD+/fs1e/ZslZeX6xvf+IZWrVpV78IEAMDpy+ecc9ZNfFkoFFIgELBuAwBwioLBoOLj4xtdb34VHADg9EQAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADARAfrBoCWZPTo0Z5rBg8e7LnmN7/5jeeauro6zzXN6f777/dc8+ijj8agE7QWnAEBAEwQQAAAE1EPoLlz58rn80WMfv36RXszAIBWLibvAV188cV68803/28jHXirCQAQKSbJ0KFDB6WkpMTiqQEAbURM3gPasWOH0tLSlJGRoR/84AfauXNno3NramoUCoUiBgCg7Yt6AGVmZqqgoECrVq3SM888o7KyMl1xxRU6dOhQg/Pz8vIUCATCo0ePHtFuCQDQAkU9gHJycvT9739fAwcOVHZ2tv7yl7/o4MGDeuWVVxqcP3PmTAWDwfDYtWtXtFsCALRAMb86oFu3bjr//PNVUlLS4Hq/3y+/3x/rNgAALUzM/w7o8OHDKi0tVWpqaqw3BQBoRaIeQPfcc4/Wrl2rjz/+WP/+9781fvx4tW/fXjfddFO0NwUAaMWi/hLc7t27ddNNN6myslJnn322Lr/8chUXF+vss8+O9qYAAK2YzznnrJv4slAopEAgYN0GTlPXXHON55ply5Z5rvH5fJ5rWtiPaj21tbWeaz788EPPNWPHjvVc89FHH3muwakLBoOKj49vdD33ggMAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGAi5h9IB3xZYmKi55qJEyd6rrnzzjs910hqthvhbtu2zXPN5s2bPdds2bLFc40kLV261HPN1KlTPddMmjTJc01xcbHnmurqas81klRTU+O5Zvz48Z5rmnI8tAWcAQEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATHA3bDTZbbfd5rnm5z//ueeaXr16ea5pqnfeecdzzXPPPee5ZtmyZZ5r9u/f77mmOT355JOea66++mrPNQMGDPBc01QbN270XLN3794YdNI2cQYEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABDcjbWN++tOfeq65+eabm7StCy+80HNN165dm7Qtrx588MEm1T377LOea/bt29ekbbVkSUlJnmtuueUWzzVNubFoVVWV55q///3vnmskKTc313NNZWVlk7Z1OuIMCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAluRtqCTZs2zXNNXl6e5xq/3++5pqnefPNNzzWPPPKI55r333/fc43UNm8s2hSXXXaZ55qHH344Bp3Ut379es813//+92PQCU4VZ0AAABMEEADAhOcAWrduna655hqlpaXJ5/Np+fLlEeudc5o9e7ZSU1PVpUsXZWVlaceOHdHqFwDQRngOoKqqKg0aNEj5+fkNrp83b56eeOIJPfvss9qwYYO6du2q7OxsVVdXn3KzAIC2w/NFCDk5OcrJyWlwnXNO8+fP1y9/+UuNHTtWkrRw4UIlJydr+fLluvHGG0+tWwBAmxHV94DKyspUXl6urKys8LJAIKDMzMxGr1ypqalRKBSKGACAti+qAVReXi5JSk5OjlienJwcXvdVeXl5CgQC4dGjR49otgQAaKHMr4KbOXOmgsFgeOzatcu6JQBAM4hqAKWkpEiSKioqIpZXVFSE132V3+9XfHx8xAAAtH1RDaD09HSlpKSosLAwvCwUCmnDhg0aNmxYNDcFAGjlPF8Fd/jwYZWUlIQfl5WVacuWLUpISFDPnj1111136Ve/+pX69u2r9PR0zZo1S2lpaRo3blw0+wYAtHKeA2jTpk268sorw49nzJghSZo4caIKCgp03333qaqqSrfffrsOHjyoyy+/XKtWrVLnzp2j1zUAoNXzOeecdRNfFgqFFAgErNuIuilTpniumT9/vueaDh2a7/6y999/v+eahQsXeq7Zv3+/5xqcmttvv91zzdNPP+255rPPPvNc05RXU778tgCaTzAYPOH7+uZXwQEATk8EEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABPNd+vk01x+fr7nmhZ2o/J63nvvPc81Pp8vBp2gMWeccUaT6u6+++4od9KwuXPneq7hztZtB2dAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATHAz0maydOlSzzXXXnut55rmvNnnypUrPdfs27fPc01T9l1L949//MNzzd/+9jfPNS+88ILnGknq3bu355qtW7d6rnn55Zc916Dt4AwIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACZ9zzlk38WWhUEiBQMC6jRZh+vTpnmsyMzM911x33XWea3BqmnLT2Bb2o1rP0KFDPdds2rQpBp2gpQgGg4qPj290PWdAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATHAzUjRZnz59PNf86Ec/ikEn9V199dVNquvfv3+UO2lYu3be/+9XV1cXg06ip7S01HPN6NGjPdd8/PHHnmtgg5uRAgBaJAIIAGDCcwCtW7dO11xzjdLS0uTz+bR8+fKI9ZMmTZLP54sYY8aMiVa/AIA2wnMAVVVVadCgQcrPz290zpgxY7R3797wWLJkySk1CQBoezp4LcjJyVFOTs4J5/j9fqWkpDS5KQBA2xeT94CKioqUlJSkCy64QFOnTlVlZWWjc2tqahQKhSIGAKDti3oAjRkzRgsXLlRhYaEeeeQRrV27Vjk5OaqtrW1wfl5engKBQHj06NEj2i0BAFogzy/BncyNN94Y/nrAgAEaOHCgevfuraKiIo0aNare/JkzZ2rGjBnhx6FQiBACgNNAzC/DzsjIUGJiokpKShpc7/f7FR8fHzEAAG1fzANo9+7dqqysVGpqaqw3BQBoRTy/BHf48OGIs5mysjJt2bJFCQkJSkhI0AMPPKAJEyYoJSVFpaWluu+++9SnTx9lZ2dHtXEAQOvmOYA2bdqkK6+8Mvz4i/dvJk6cqGeeeUZbt27VH//4Rx08eFBpaWkaPXq0HnroIfn9/uh1DQBo9bgZKdqkzp07N6muY8eOnmuGDh3quWbVqlWea1rYj2pU7Nixw3PNgw8+6LmGP4a3wc1IAQAtEgEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADARNQ/khtoCaqrq5tUV1tb67lm2rRpTdpWc/jFL37RpLrNmzd7rnnxxRc91/Tt29dzzbx58zzXbNy40XONJJWWljapDl8PZ0AAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBM+JxzzrqJLwuFQgoEAtZt4DQVHx/vuebTTz/1XOPz+TzXNOXGmEOHDvVcIzXtexo1apTnmkWLFnmuOfvssz3XvPPOO55rJGnw4MFNqsNxwWDwhD9TnAEBAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAw0cG6AaAlycjIsG6hUX/961891zTlpqJNVVhY6Llm+fLlnmsmT57suSYxMdFzDWKPMyAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmuBkp8CW33nqrdQuNeuGFF6xbiDqfz2fdAgxxBgQAMEEAAQBMeAqgvLw8XXrppYqLi1NSUpLGjRun7du3R8yprq5Wbm6uunfvrjPPPFMTJkxQRUVFVJsGALR+ngJo7dq1ys3NVXFxsd544w0dO3ZMo0ePVlVVVXjO9OnTtXLlSr366qtau3at9uzZo2uvvTbqjQMAWjdPFyGsWrUq4nFBQYGSkpK0efNmDR8+XMFgUM8//7wWL16sq666SpK0YMECXXjhhSouLtbQoUOj1zkAoFU7pfeAgsGgJCkhIUGStHnzZh07dkxZWVnhOf369VPPnj21fv36Bp+jpqZGoVAoYgAA2r4mB1BdXZ3uuusuXXbZZerfv78kqby8XJ06dVK3bt0i5iYnJ6u8vLzB58nLy1MgEAiPHj16NLUlAEAr0uQAys3N1bZt2/TSSy+dUgMzZ85UMBgMj127dp3S8wEAWocm/SHqtGnT9Prrr2vdunU699xzw8tTUlJ09OhRHTx4MOIsqKKiQikpKQ0+l9/vl9/vb0obAIBWzNMZkHNO06ZN07Jly7R69Wqlp6dHrB88eLA6duyowsLC8LLt27dr586dGjZsWHQ6BgC0CZ7OgHJzc7V48WKtWLFCcXFx4fd1AoGAunTpokAgoNtuu00zZsxQQkKC4uPjdccdd2jYsGFcAQcAiOApgJ555hlJ0siRIyOWL1iwQJMmTZIkPf7442rXrp0mTJigmpoaZWdn6+mnn45KswCAtsNTADnnTjqnc+fOys/PV35+fpObAlDfRx991GzbiouL81wzYsQIzzVjx471XIO2g3vBAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMNOkTUQE0v6Z8rEltbW2TttWjRw/PNU25G3ZTfP75555rFi5cGINOcKo4AwIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGDC55xz1k18WSgUUiAQsG4Dp6nvfOc7nmtWrFjhucbv93uuaWE/qlFx5MgRzzXz58/3XDN79mzPNTh1wWBQ8fHxja7nDAgAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJbkYKnKIRI0Z4rvnZz37muaYpN0ptqnfffddzzeLFiz3XvPTSS55rPvnkE881sMHNSAEALRIBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAAT3IwUABAT3IwUANAiEUAAABOeAigvL0+XXnqp4uLilJSUpHHjxmn79u0Rc0aOHCmfzxcxpkyZEtWmAQCtn6cAWrt2rXJzc1VcXKw33nhDx44d0+jRo1VVVRUxb/Lkydq7d294zJs3L6pNAwBavw5eJq9atSricUFBgZKSkrR582YNHz48vPyMM85QSkpKdDoEALRJp/QeUDAYlCQlJCRELH/xxReVmJio/v37a+bMmTpy5Eijz1FTU6NQKBQxAACnAddEtbW17nvf+5677LLLIpY/99xzbtWqVW7r1q1u0aJF7pxzznHjx49v9HnmzJnjJDEYDAajjY1gMHjCHGlyAE2ZMsX16tXL7dq164TzCgsLnSRXUlLS4Prq6moXDAbDY9euXeY7jcFgMBinPk4WQJ7eA/rCtGnT9Prrr2vdunU699xzTzg3MzNTklRSUqLevXvXW+/3++X3+5vSBgCgFfMUQM453XHHHVq2bJmKioqUnp5+0potW7ZIklJTU5vUIACgbfIUQLm5uVq8eLFWrFihuLg4lZeXS5ICgYC6dOmi0tJSLV68WN/97nfVvXt3bd26VdOnT9fw4cM1cODAmHwDAIBWysv7Pmrkdb4FCxY455zbuXOnGz58uEtISHB+v9/16dPH3XvvvSd9HfDLgsGg+euWDAaDwTj1cbLf/dyMFAAQE9yMFADQIhFAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATLS4AHLOWbcAAIiCk/0+b3EBdOjQIesWAABRcLLf5z7Xwk456urqtGfPHsXFxcnn80WsC4VC6tGjh3bt2qX4+HijDu2xH45jPxzHfjiO/XBcS9gPzjkdOnRIaWlpateu8fOcDs3Y09fSrl07nXvuuSecEx8ff1ofYF9gPxzHfjiO/XAc++E46/0QCAROOqfFvQQHADg9EEAAABOtKoD8fr/mzJkjv99v3Yop9sNx7Ifj2A/HsR+Oa037ocVdhAAAOD20qjMgAEDbQQABAEwQQAAAEwQQAMAEAQQAMNFqAig/P1/nnXeeOnfurMzMTG3cuNG6pWY3d+5c+Xy+iNGvXz/rtmJu3bp1uuaaa5SWliafz6fly5dHrHfOafbs2UpNTVWXLl2UlZWlHTt22DQbQyfbD5MmTap3fIwZM8am2RjJy8vTpZdeqri4OCUlJWncuHHavn17xJzq6mrl5uaqe/fuOvPMMzVhwgRVVFQYdRwbX2c/jBw5st7xMGXKFKOOG9YqAujll1/WjBkzNGfOHL311lsaNGiQsrOztW/fPuvWmt3FF1+svXv3hsc///lP65ZirqqqSoMGDVJ+fn6D6+fNm6cnnnhCzz77rDZs2KCuXbsqOztb1dXVzdxpbJ1sP0jSmDFjIo6PJUuWNGOHsbd27Vrl5uaquLhYb7zxho4dO6bRo0erqqoqPGf69OlauXKlXn31Va1du1Z79uzRtddea9h19H2d/SBJkydPjjge5s2bZ9RxI1wrMGTIEJebmxt+XFtb69LS0lxeXp5hV81vzpw5btCgQdZtmJLkli1bFn5cV1fnUlJS3G9/+9vwsoMHDzq/3++WLFli0GHz+Op+cM65iRMnurFjx5r0Y2Xfvn1Oklu7dq1z7vi/fceOHd2rr74anvP+++87SW79+vVWbcbcV/eDc86NGDHC3XnnnXZNfQ0t/gzo6NGj2rx5s7KyssLL2rVrp6ysLK1fv96wMxs7duxQWlqaMjIy9IMf/EA7d+60bslUWVmZysvLI46PQCCgzMzM0/L4KCoqUlJSki644AJNnTpVlZWV1i3FVDAYlCQlJCRIkjZv3qxjx45FHA/9+vVTz5492/Tx8NX98IUXX3xRiYmJ6t+/v2bOnKkjR45YtNeoFnc37K86cOCAamtrlZycHLE8OTlZH3zwgVFXNjIzM1VQUKALLrhAe/fu1QMPPKArrrhC27ZtU1xcnHV7JsrLyyWpwePji3WnizFjxujaa69Venq6SktL9fOf/1w5OTlav3692rdvb91e1NXV1emuu+7SZZddpv79+0s6fjx06tRJ3bp1i5jblo+HhvaDJN18883q1auX0tLStHXrVt1///3avn27XnvtNcNuI7X4AML/ycnJCX89cOBAZWZmqlevXnrllVd02223GXaGluDGG28Mfz1gwAANHDhQvXv3VlFRkUaNGmXYWWzk5uZq27Ztp8X7oCfS2H64/fbbw18PGDBAqampGjVqlEpLS9W7d+/mbrNBLf4luMTERLVv377eVSwVFRVKSUkx6qpl6Natm84//3yVlJRYt2Lmi2OA46O+jIwMJSYmtsnjY9q0aXr99de1Zs2aiM8PS0lJ0dGjR3Xw4MGI+W31eGhsPzQkMzNTklrU8dDiA6hTp04aPHiwCgsLw8vq6upUWFioYcOGGXZm7/DhwyotLVVqaqp1K2bS09OVkpIScXyEQiFt2LDhtD8+du/ercrKyjZ1fDjnNG3aNC1btkyrV69Wenp6xPrBgwerY8eOEcfD9u3btXPnzjZ1PJxsPzRky5YtktSyjgfrqyC+jpdeesn5/X5XUFDg3nvvPXf77be7bt26ufLycuvWmtXdd9/tioqKXFlZmfvXv/7lsrKyXGJiotu3b591azF16NAh9/bbb7u3337bSXK/+93v3Ntvv+0++eQT55xzDz/8sOvWrZtbsWKF27p1qxs7dqxLT093n332mXHn0XWi/XDo0CF3zz33uPXr17uysjL35ptvum9961uub9++rrq62rr1qJk6daoLBAKuqKjI7d27NzyOHDkSnjNlyhTXs2dPt3r1ardp0yY3bNgwN2zYMMOuo+9k+6GkpMQ9+OCDbtOmTa6srMytWLHCZWRkuOHDhxt3HqlVBJBzzj355JOuZ8+erlOnTm7IkCGuuLjYuqVmd8MNN7jU1FTXqVMnd84557gbbrjBlZSUWLcVc2vWrHGS6o2JEyc6545fij1r1iyXnJzs/H6/GzVqlNu+fbtt0zFwov1w5MgRN3r0aHf22We7jh07ul69ernJkye3uf+kNfT9S3ILFiwIz/nss8/cT37yE3fWWWe5M844w40fP97t3bvXrukYONl+2Llzpxs+fLhLSEhwfr/f9enTx917770uGAzaNv4VfB4QAMBEi38PCADQNhFAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADAxP8DahwKnrBSbewAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Combine training and test datasets to get the full dataset\n",
        "x_full = np.concatenate((x_train, x_test), axis=0)\n",
        "y_full = np.concatenate((y_train, y_test), axis=0)\n",
        "\n",
        "\n",
        "# Function to select a specified number of samples for each category\n",
        "def select_samples(x, y, num_samples):\n",
        "    selected_samples = []\n",
        "    selected_labels = []\n",
        "    for i in range(10):  # 10 categories in MNIST\n",
        "        indices = np.where(y == i)[0][:num_samples]\n",
        "        selected_samples.append(x[indices])\n",
        "        selected_labels.append(y[indices])\n",
        "    selected_samples = np.concatenate(selected_samples, axis=0)\n",
        "    selected_labels = np.concatenate(selected_labels, axis=0)\n",
        "    return selected_samples, selected_labels\n",
        "\n",
        "# Select 1,000 samples for each category\n",
        "x_subset, y_subset = select_samples(x_full, y_full, 1000)\n",
        "\n",
        "# Display one random image from x_train_subset\n",
        "index = np.random.randint(0, len(x_subset))\n",
        "image = x_subset[index]\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.title(f\"Label: {y_subset[index]}\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Split the subset into training and testing sets\n",
        "x_train_subset, x_test_subset, y_train_subset, y_test_subset = train_test_split(\n",
        "    x_subset, y_subset, test_size=0.2, random_state=42\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "labeled_mask = np.zeros(10000, dtype=bool)\n",
        "for i in range(10):\n",
        "    indices = np.where(np.array(y_subset) == i)[0] # returns shuffled array of indices of each class, total 1000 data.\n",
        "    np.random.shuffle(indices)\n",
        "    labeled_mask[indices[:10]] = True\n",
        "\n",
        "\n",
        "print(np.where(labeled_mask)[0])\n",
        "\n",
        "labeled_dataset_to_be_trained_index = np.where(labeled_mask)[0]\n",
        "unlabeled_dataset_index = np.where(~labeled_mask)[0]\n",
        "\n",
        "\n",
        "\n",
        "labeled_dataset_to_be_trained_x = np.array([x_subset[index]  for index in labeled_dataset_to_be_trained_index])\n",
        "labeled_dataset_to_be_trained_y = np.array([y_subset[index]  for index in labeled_dataset_to_be_trained_index])\n",
        "\n",
        "unlabeled_dataset_x = np.array([x_subset[index]  for index in unlabeled_dataset_index])\n",
        "# unlabeled_dataset_y = np.array([y_subset[index]  for index in unlabeled_dataset_index])\n",
        "\n",
        "print(len(labeled_dataset_to_be_trained_y))\n",
        "\n",
        "unique_labels = np.unique([y for y in labeled_dataset_to_be_trained_y])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Display one random image from x_train_subset\n",
        "index = np.random.randint(0, len(labeled_dataset_to_be_trained_x))\n",
        "image = labeled_dataset_to_be_trained_x[index]\n",
        "\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.title(f\"Label: {labeled_dataset_to_be_trained_y[index]}\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# train_labels_one_hot_y = keras.utils.to_categorical(unique_labels, num_classes=10)\n",
        "\n",
        "# Split the subset into training and testing sets\n",
        "labeled_x_train_subset,labeled_x_test_subset, labeled_y_train_subset, labeled_y_test_subset = train_test_split(\n",
        "    labeled_dataset_to_be_trained_x, labeled_dataset_to_be_trained_y, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "train_labels_y = keras.utils.to_categorical(labeled_y_train_subset, num_classes=10)\n",
        "test_labels_y = keras.utils.to_categorical(labeled_y_test_subset, num_classes=10)\n",
        "\n",
        "print(train_labels_y.shape, 'akfdkafdkjh', labeled_y_train_subset.shape, labeled_dataset_to_be_trained_y.shape)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "UsNNtoSwzzp0",
        "outputId": "02d73f63-3db4-4970-c7e7-d992890ecfce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  27  231  298  306  553  672  706  859  971  993 1018 1201 1276 1386\n",
            " 1506 1532 1601 1671 1845 1939 2070 2071 2181 2222 2489 2500 2676 2680\n",
            " 2842 2947 3228 3277 3325 3329 3350 3503 3582 3587 3625 3834 4027 4081\n",
            " 4399 4636 4727 4756 4796 4797 4820 4947 5222 5495 5604 5647 5653 5853\n",
            " 5859 5894 5935 5985 6050 6128 6273 6401 6420 6516 6538 6623 6757 6775\n",
            " 7331 7376 7384 7461 7522 7532 7625 7647 7799 7848 8027 8221 8224 8242\n",
            " 8385 8428 8762 8777 8854 8996 9009 9181 9292 9712 9719 9748 9804 9838\n",
            " 9985 9990]\n",
            "100\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(90, 10) akfdkafdkjh (90,) (100,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the neural network model\n",
        "model = keras.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28,1)),  # Assuming input images are 28x28\n",
        "    layers.Dense(200, activation='relu'),   # Hidden layer with 200 neurons and ReLU activation\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "# model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "# model.fit(labeled_x_train_subset, train_labels_y , epochs=20, batch_size=64, validation_data=(labeled_x_test_subset, test_labels_y))\n",
        "model.fit(labeled_x_train_subset, train_labels_y , epochs=100, batch_size=20)\n",
        "\n",
        "print(labeled_x_test_subset.shape)\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(labeled_x_test_subset, test_labels_y)\n",
        "test_loss_ul, test_acc_ul = model.evaluate(unlabeled_dataset_x)\n",
        "\n",
        "print('\\nTest accuracy:', test_acc)\n",
        "print('\\nTest accuracy for unlabeled data set:', test_acc_ul)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zMQPODd5fYK",
        "outputId": "4172adca-a5af-4d0a-c58a-2fb46f50b01b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "5/5 [==============================] - 2s 9ms/step - loss: 166.5521 - accuracy: 0.1778\n",
            "Epoch 2/100\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 105.5731 - accuracy: 0.3111\n",
            "Epoch 3/100\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 49.9916 - accuracy: 0.4778\n",
            "Epoch 4/100\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 35.2217 - accuracy: 0.5778\n",
            "Epoch 5/100\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 24.5506 - accuracy: 0.6667\n",
            "Epoch 6/100\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 15.0060 - accuracy: 0.7222\n",
            "Epoch 7/100\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 11.3006 - accuracy: 0.8333\n",
            "Epoch 8/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 12.9317 - accuracy: 0.7556\n",
            "Epoch 9/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 7.8849 - accuracy: 0.8556\n",
            "Epoch 10/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 7.5032 - accuracy: 0.8667\n",
            "Epoch 11/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 4.0958 - accuracy: 0.8667\n",
            "Epoch 12/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 4.7384 - accuracy: 0.8556\n",
            "Epoch 13/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 3.6165 - accuracy: 0.8889\n",
            "Epoch 14/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 2.8055 - accuracy: 0.8889\n",
            "Epoch 15/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 3.0106 - accuracy: 0.9222\n",
            "Epoch 16/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 2.6535 - accuracy: 0.9111\n",
            "Epoch 17/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 1.4643 - accuracy: 0.9222\n",
            "Epoch 18/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 5.0978 - accuracy: 0.9000\n",
            "Epoch 19/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 2.0739 - accuracy: 0.9444\n",
            "Epoch 20/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 1.6090 - accuracy: 0.9222\n",
            "Epoch 21/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 2.6851 - accuracy: 0.9111\n",
            "Epoch 22/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 2.7288 - accuracy: 0.9444\n",
            "Epoch 23/100\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 2.6665 - accuracy: 0.9222\n",
            "Epoch 24/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.7182 - accuracy: 0.9556\n",
            "Epoch 25/100\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 2.3143 - accuracy: 0.9222\n",
            "Epoch 26/100\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0497 - accuracy: 0.9778\n",
            "Epoch 27/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 2.0660 - accuracy: 0.9333\n",
            "Epoch 28/100\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.2683 - accuracy: 0.9778\n",
            "Epoch 29/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.1259 - accuracy: 0.9889\n",
            "Epoch 30/100\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.0301 - accuracy: 0.9889\n",
            "Epoch 31/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.9152 - accuracy: 0.9667\n",
            "Epoch 32/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.4407 - accuracy: 0.9889\n",
            "Epoch 33/100\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.1890 - accuracy: 0.9778\n",
            "Epoch 34/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.6257 - accuracy: 0.9444\n",
            "Epoch 35/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.7953 - accuracy: 0.9556\n",
            "Epoch 36/100\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.2350 - accuracy: 0.9889\n",
            "Epoch 37/100\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0493 - accuracy: 0.9889\n",
            "Epoch 38/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.1322 - accuracy: 0.9889\n",
            "Epoch 39/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.7558 - accuracy: 0.9556\n",
            "Epoch 40/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.3171 - accuracy: 0.9667\n",
            "Epoch 41/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.3030 - accuracy: 0.9778\n",
            "Epoch 42/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 1.1321 - accuracy: 0.9556\n",
            "Epoch 43/100\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.1485 - accuracy: 0.9778\n",
            "Epoch 44/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.5524 - accuracy: 0.9778\n",
            "Epoch 45/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.2886 - accuracy: 0.9778\n",
            "Epoch 46/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.5918 - accuracy: 0.9778\n",
            "Epoch 47/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.1942 - accuracy: 0.9667\n",
            "Epoch 48/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 9.0222e-06 - accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 1.0264 - accuracy: 0.9556\n",
            "Epoch 50/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 8.9844e-04 - accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 1.0883 - accuracy: 0.9556\n",
            "Epoch 52/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.6473 - accuracy: 0.9778\n",
            "Epoch 53/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.4861 - accuracy: 0.9667\n",
            "Epoch 54/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.7295 - accuracy: 0.9667\n",
            "Epoch 55/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 3.9736e-09 - accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.8272 - accuracy: 0.9778\n",
            "Epoch 57/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.0565 - accuracy: 0.9889\n",
            "Epoch 58/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.0746 - accuracy: 0.9889\n",
            "Epoch 59/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.4879 - accuracy: 0.9778\n",
            "Epoch 60/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 5.6671e-04 - accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.6495 - accuracy: 0.9444\n",
            "Epoch 62/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.1832 - accuracy: 0.9778\n",
            "Epoch 63/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 1.1335 - accuracy: 0.9444\n",
            "Epoch 64/100\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.1677 - accuracy: 0.9889\n",
            "Epoch 65/100\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 1.7441e-04 - accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "5/5 [==============================] - 0s 23ms/step - loss: 0.2681 - accuracy: 0.9889\n",
            "Epoch 67/100\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.1592 - accuracy: 0.9778\n",
            "Epoch 68/100\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 6.1192e-07 - accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 0.3459 - accuracy: 0.9778\n",
            "Epoch 70/100\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 0.0848 - accuracy: 0.9889\n",
            "Epoch 71/100\n",
            "5/5 [==============================] - 0s 24ms/step - loss: 0.0984 - accuracy: 0.9889\n",
            "Epoch 72/100\n",
            "5/5 [==============================] - 0s 27ms/step - loss: 0.0721 - accuracy: 0.9889\n",
            "Epoch 73/100\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.4860 - accuracy: 0.9556\n",
            "Epoch 74/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.0231 - accuracy: 0.9889\n",
            "Epoch 75/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.5954 - accuracy: 0.9667\n",
            "Epoch 76/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.6017 - accuracy: 0.9778\n",
            "Epoch 77/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 2.1351 - accuracy: 0.9333\n",
            "Epoch 78/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.2847 - accuracy: 0.9778\n",
            "Epoch 79/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.0124 - accuracy: 0.9889\n",
            "Epoch 80/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 3.9736e-09 - accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.1281 - accuracy: 0.9889\n",
            "Epoch 82/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3413 - accuracy: 0.9778\n",
            "Epoch 83/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 1.7219e-08 - accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.7321 - accuracy: 0.9667\n",
            "Epoch 85/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.6204 - accuracy: 0.9889\n",
            "Epoch 86/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.5718 - accuracy: 0.9889\n",
            "Epoch 87/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.9847 - accuracy: 0.9889\n",
            "Epoch 88/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 1.3977 - accuracy: 0.9556\n",
            "Epoch 89/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.5755 - accuracy: 0.9889\n",
            "Epoch 90/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.4460 - accuracy: 0.9667\n",
            "Epoch 91/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 1.3229 - accuracy: 0.9444\n",
            "Epoch 92/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.7770 - accuracy: 0.9778\n",
            "Epoch 93/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.4674 - accuracy: 0.9778\n",
            "Epoch 95/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.9547 - accuracy: 0.9667\n",
            "Epoch 96/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.4080 - accuracy: 0.9556\n",
            "Epoch 97/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.7160 - accuracy: 0.9667\n",
            "Epoch 98/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.2772 - accuracy: 0.9667\n",
            "Epoch 99/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.2184 - accuracy: 0.9778\n",
            "Epoch 100/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.5820 - accuracy: 0.9667\n",
            "(10, 28, 28)\n",
            "1/1 [==============================] - 0s 290ms/step - loss: 23.5540 - accuracy: 0.5000\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00\n",
            "\n",
            "Test accuracy: 0.5\n",
            "\n",
            "Test accuracy for unlabeled data set: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Virtual Adversarial Training\n",
        "x = labeled_x_train_subset\n",
        "\n",
        "def generate_random_unit_vector(input):\n",
        "    x = np.random.normal(0, 1, input.shape)\n",
        "    d = x / np.linalg.norm(x)\n",
        "    return d\n",
        "\n",
        "\n",
        "r = tf.random.normal(shape=tf.shape(labeled_x_train_subset))\n",
        "# r =  generate_random_unit_vector(r)\n",
        "perturbed_input = labeled_x_train_subset+0.01*r\n",
        "\n",
        "model_vat = keras.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28,1)),  # Assuming input images are 28x28\n",
        "    layers.Dense(200, activation='relu'),   # Hidden layer with 200 neurons and ReLU activation\n",
        "    layers.Dense(10, activation='softmax')\n",
        "\n",
        "])\n",
        "\n",
        "alpha = 0.001  # A hyperparameter for controlling the strength of the perturbation\n",
        "optimizer = keras.optimizers.Adam()\n",
        "model_vat.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "def virtual_adversarial_loss(x, logits):\n",
        "    d = generate_random_unit_vector(x)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(x)\n",
        "        logits_perturbed = model_vat(x + alpha * d)\n",
        "        loss = tf.keras.losses.categorical_crossentropy(logits, logits_perturbed)\n",
        "\n",
        "    r_vadv = tape.gradient(loss, x)\n",
        "    r_vadv_normalized = alpha * r_vadv / tf.norm(r_vadv)\n",
        "\n",
        "    logits_perturbed = model_vat(x + r_vadv_normalized)\n",
        "    loss = tf.keras.losses.categorical_crossentropy(logits, logits_perturbed)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i in range(0, len(perturbed_input), batch_size):\n",
        "        x_batch = perturbed_input[i:i+batch_size]\n",
        "        y_batch = train_labels_y[i:i+batch_size]\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model_vat(x_batch)\n",
        "            classification_loss = tf.keras.losses.categorical_crossentropy(y_batch, logits)\n",
        "            vat_loss = virtual_adversarial_loss(x_batch, logits)\n",
        "            total_loss = classification_loss + vat_loss\n",
        "        gradients = tape.gradient(total_loss, model_vat.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model_vat.trainable_variables))\n",
        "\n",
        "    if i % 40 == 0:\n",
        "              mean_total_loss = tf.reduce_mean(total_loss).numpy()\n",
        "              # print(mean_total_loss, )\n",
        "              print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/80] Total Loss: {mean_total_loss:.4f}')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model_vat.evaluate(labeled_x_test_subset, test_labels_y)\n",
        "print(\"Test accuracy for VAT:\", test_acc)\n",
        "# Evaluate the model on the unlabeled data set, first use mode to predict the logit\n",
        "\n",
        "unlabeled_predictions = model_vat.predict(unlabeled_dataset_x)\n",
        "unlabeled_predictions_exact = np.argmax(unlabeled_predictions, axis=1)\n",
        "unlabeled_dataset_y_for_vat = np.array([y_subset[index]  for index in unlabeled_dataset_index])\n",
        "\n",
        "print(unlabeled_predictions_exact.shape, 'shape', unlabeled_dataset_y_for_vat.shape)\n",
        "accuracy_unlabeled = np.mean(unlabeled_predictions_exact == unlabeled_dataset_y_for_vat)\n",
        "print(\"Accuracy on unlabeled dataset:\", accuracy_unlabeled)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVKj5jlQANhP",
        "outputId": "c55254ca-c2df-477e-cb9f-f9142ac25aed"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Step [81/80] Total Loss: 79.8332\n",
            "Epoch [2/20], Step [81/80] Total Loss: 13.6079\n",
            "Epoch [3/20], Step [81/80] Total Loss: 5.7493\n",
            "Epoch [4/20], Step [81/80] Total Loss: 1.2135\n",
            "Epoch [5/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [6/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [7/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [8/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [9/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [10/20], Step [81/80] Total Loss: 0.0001\n",
            "Epoch [11/20], Step [81/80] Total Loss: 0.0002\n",
            "Epoch [12/20], Step [81/80] Total Loss: 0.0002\n",
            "Epoch [13/20], Step [81/80] Total Loss: 0.0002\n",
            "Epoch [14/20], Step [81/80] Total Loss: 0.0002\n",
            "Epoch [15/20], Step [81/80] Total Loss: 0.0001\n",
            "Epoch [16/20], Step [81/80] Total Loss: 0.0001\n",
            "Epoch [17/20], Step [81/80] Total Loss: 0.0001\n",
            "Epoch [18/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [19/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [20/20], Step [81/80] Total Loss: 0.0000\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 44.8181 - accuracy: 0.7000\n",
            "Test accuracy for VAT: 0.699999988079071\n",
            "310/310 [==============================] - 1s 2ms/step\n",
            "(9900,) shape (9900,)\n",
            "Accuracy on unlabeled dataset: 0.6406060606060606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entropy minimization\n",
        "x = labeled_x_train_subset\n",
        "\n",
        "model_entropy_minimization = keras.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28,1)),  # Assuming input images are 28x28\n",
        "    layers.Dense(200, activation='relu'),   # Hidden layer with 200 neurons and ReLU activation\n",
        "    layers.Dense(10, activation='softmax')\n",
        "\n",
        "])\n",
        "\n",
        "alpha = 0.001  # A hyperparameter for controlling the strength of the perturbation\n",
        "optimizer = keras.optimizers.Adam()\n",
        "model_entropy_minimization.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "def entropy_minimization_loss(y_true, y_pred, alpha=0.001):\n",
        "    # Calculating the standard cross-entropy loss\n",
        "    # print(\"INSIDE ENTROPY MINIMIZATION LOSS\", y_true, y_pred)\n",
        "    cross_entropy_loss = categorical_crossentropy(y_true, y_pred)\n",
        "    # print(cross_entropy_loss.shape)\n",
        "\n",
        "\n",
        "    # Calculating the entropy of the predicted probabilities\n",
        "    epsilon = 1e-10\n",
        "    entropy = tf.reduce_sum(-y_pred * tf.math.log(y_pred + epsilon), axis=1)\n",
        "    # print('afdkafjk', entropy)\n",
        "\n",
        "    # Combining the cross-entropy loss with the entropy regularization term\n",
        "    total_loss = cross_entropy_loss + alpha * entropy\n",
        "\n",
        "    # tf.debugging.check_numerics(cross_entropy_loss, \"cross_entropy_loss is NaN or Inf\")\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i in range(0, len(x), batch_size):\n",
        "        x_batch = x[i:i+batch_size]\n",
        "        y_batch = train_labels_y[i:i+batch_size]\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model_entropy_minimization(x_batch)\n",
        "            total_loss = entropy_minimization_loss(y_batch,logits )\n",
        "        gradients = tape.gradient(total_loss, model_entropy_minimization.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model_entropy_minimization.trainable_variables))\n",
        "\n",
        "    if i % 40 == 0:\n",
        "              mean_total_loss = tf.reduce_mean(total_loss).numpy()\n",
        "              # print(mean_total_loss, )\n",
        "              print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/80] Total Loss: {mean_total_loss:.4f}')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model_entropy_minimization.evaluate(labeled_x_test_subset, test_labels_y)\n",
        "print(\"Test accuracy for Entropy minimization:\", test_acc)\n",
        "# Evaluate the model on the unlabeled data set, first use mode to predict the logit\n",
        "\n",
        "unlabeled_predictions = model_entropy_minimization.predict(unlabeled_dataset_x)\n",
        "unlabeled_predictions_exact = np.argmax(unlabeled_predictions, axis=1)\n",
        "unlabeled_dataset_y_for_entropy_minimization = np.array([y_subset[index]  for index in unlabeled_dataset_index])\n",
        "\n",
        "print(unlabeled_predictions_exact.shape, 'shape', unlabeled_dataset_y_for_entropy_minimization.shape)\n",
        "accuracy_unlabeled = np.mean(unlabeled_predictions_exact == unlabeled_dataset_y_for_entropy_minimization)\n",
        "print(\"Accuracy on unlabeled dataset for entropy minimization:\", accuracy_unlabeled)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umh2yWQzE7oy",
        "outputId": "3d5c9752-0c12-428c-b0de-eecda230af7e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Step [81/80] Total Loss: 95.2384\n",
            "Epoch [2/20], Step [81/80] Total Loss: 20.0099\n",
            "Epoch [3/20], Step [81/80] Total Loss: 9.9000\n",
            "Epoch [4/20], Step [81/80] Total Loss: 4.7003\n",
            "Epoch [5/20], Step [81/80] Total Loss: 0.2726\n",
            "Epoch [6/20], Step [81/80] Total Loss: 0.5209\n",
            "Epoch [7/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [8/20], Step [81/80] Total Loss: 0.0002\n",
            "Epoch [9/20], Step [81/80] Total Loss: 0.0004\n",
            "Epoch [10/20], Step [81/80] Total Loss: 0.0003\n",
            "Epoch [11/20], Step [81/80] Total Loss: 0.0001\n",
            "Epoch [12/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [13/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [14/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [15/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [16/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [17/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [18/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [19/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [20/20], Step [81/80] Total Loss: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 314 calls to <function Model.make_test_function.<locals>.test_function at 0x7c340d0f7f40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 243ms/step - loss: 35.7277 - accuracy: 0.5000\n",
            "Test accuracy for VAT: 0.5\n",
            "310/310 [==============================] - 1s 2ms/step\n",
            "(9900,) shape (9900,)\n",
            "Accuracy on unlabeled dataset: 0.6366666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entropy minimization\n",
        "# Log likelihood of labelled data\n",
        "\n",
        "# labeled_x_train_subset, train_labels_y\n",
        "\n",
        "# Just for checking\n",
        "# print(train_labels_y[0])\n",
        "# image = labeled_x_train_subset[0]\n",
        "# # Display the image\n",
        "# plt.imshow(image, cmap='gray')\n",
        "# plt.title(f\"Label: {train_labels_y[0]}\")\n",
        "# plt.show()\n",
        "# just for checking single input\n",
        "# single_input = np.reshape(labeled_x_test_subset[0], (1, 28, 28))\n",
        "# prediction = model.predict(single_input)\n",
        "# print(np.argmax(prediction))\n",
        "# LOG LIKELIHOOD OF LABELLED DATA\n",
        "# dot product of labeled one hot vector & prediction and then finding log of each data and then finding sum.\n",
        "\n",
        "# predictions_y = model.predict(labeled_x_train_subset)\n",
        "# print(predictions_y)\n",
        "# dot_product_of_one_hot_vector_predictions = np.dot(train_labels_y.T, predictions_y)\n",
        "# epsilon = 1e-10\n",
        "# dot_product_result = np.maximum(dot_product_of_one_hot_vector_predictions, epsilon)\n",
        "# log_result = np.log(dot_product_result)\n",
        "# result = np.sum(log_result, axis=1)\n",
        "# print(\"HKJFDHAKJFHDSJKFHAJSFHEUWHUI\", log_result, result)\n",
        "\n",
        "\n",
        "# sum_of_all_categories_for_ith_data = np.sum(np.log(train_labels_y.T, predictions_y))\n",
        "# print(sum_of_all_categories_for_ith_data)\n",
        "# max_predictions_value = np.argmax(predictions)\n",
        "# print(max_predictions_value,'fjahjfhdalfj')\n",
        "# print(predictions_y.shape, np.dot(train_labels_y.T, predictions_y).shape,  np.log(np.dot(train_labels_y.T, predictions_y)))\n",
        "# output_dot_product = np.dot(train_labels_y.T, predictions_y)\n",
        "# print(output_dot_product.shape,'fkhdsakjfhdajklsh')\n",
        "\n",
        "\n",
        "def entropy_minimization_loss(y_true, y_pred, alpha=0.001):\n",
        "    # Calculating the standard cross-entropy loss\n",
        "    print(\"INSIDE ENTROPY MINIMIZATION LOSS\", y_true, y_pred)\n",
        "    cross_entropy_loss = categorical_crossentropy(y_true, y_pred)\n",
        "    print(cross_entropy_loss.shape)\n",
        "\n",
        "\n",
        "    # Calculating the entropy of the predicted probabilities\n",
        "    entropy = tf.reduce_sum(-y_pred * tf.math.log(y_pred), axis=1)\n",
        "    print('afdkafjk', entropy)\n",
        "\n",
        "    # Combining the cross-entropy loss with the entropy regularization term\n",
        "    total_loss = cross_entropy_loss + alpha * entropy\n",
        "\n",
        "    # tf.debugging.check_numerics(cross_entropy_loss, \"cross_entropy_loss is NaN or Inf\")\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "\n",
        "model_entropy_minimization = keras.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28,1)),  # Assuming input images are 28x28\n",
        "    layers.Dense(200, activation='relu'),   # Hidden layer with 200 neurons and ReLU activation\n",
        "    layers.Dense(10, activation='softmax')\n",
        "\n",
        "])\n",
        "model_entropy_minimization.compile(optimizer='adam', loss=entropy_minimization_loss, metrics=['accuracy'])\n",
        "model_entropy_minimization.fit(labeled_x_train_subset, train_labels_y, epochs=20, batch_size=64)\n",
        "\n",
        "test_loss, test_acc = model_entropy_minimization.evaluate(labeled_x_test_subset, test_labels_y)\n",
        "print(\"Test accuracy for entropy minimization\", test_acc)\n",
        "\n",
        "# optimizer = Adam(clipvalue=1.0)  # You can adjust the clip value\n",
        "# model.compile(optimizer=optimizer, loss=entropy_minimization_loss, metrics=['accuracy'])\n",
        "# model.compile(optimizer='adam', loss=entropy_minimization_loss, metrics=['accuracy'])\n",
        "\n",
        "print(\"NaNs in input data:\", np.isnan(labeled_x_train_subset).any())\n",
        "print(\"Infinities in input data:\", np.isinf(labeled_x_train_subset).any())\n",
        "\n",
        "# Check for NaN or infinite values in labels\n",
        "print(\"NaNs in labels:\", np.isnan(train_labels_y).any())\n",
        "print(\"Infinities in labels:\", np.isinf(train_labels_y).any())\n",
        "\n",
        "# model.fit(labeled_x_train_subset, train_labels_y, epochs=20, batch_size=64)\n",
        "\n",
        "\n",
        "\n",
        "# predictions_y = model.predict(labeled_x_train_subset)\n",
        "# dot_product_of_one_hot_vector_predictions = np.dot(train_labels_y.T, predictions_y)\n",
        "# epsilon = 1e-10\n",
        "# dot_product_result = np.maximum(dot_product_of_one_hot_vector_predictions, epsilon)\n",
        "# log_result = np.log(dot_product_result)\n",
        "# result = np.sum(log_result, axis=1)\n",
        "\n",
        "\n",
        "# For unlabelled data\n",
        "# unlabeled_datas_y = np.ones((9900,10))\n",
        "# predictions_unlabeled_y = model.predict(unlabeled_dataset_x)\n",
        "\n",
        "# test_loss, test_acc = model.evaluate(unlabeled_dataset_x, predictions_unlabeled_y)\n",
        "# print('test accuracy', test_acc)\n",
        "\n",
        "# print(predictions_unlabeled_y.shape, 'shape of unlabeled')\n",
        "# log_of_each_predictions = np.log(predictions_unlabeled_y)\n",
        "# print('afdkjashfdjahfjdhfajhdjfd', log_of_each_predictions)\n",
        "# print(labeled_x_test_subset.shape)\n",
        "# Evaluate the model on the test set\n",
        "# print(log_of_each_predictions.shape)\n",
        "# test_loss, test_acc = model.evaluate(unlabeled_dataset_x, log_of_each_predictions)\n",
        "# print('test accuracy', test_acc)\n",
        "\n",
        "# epsilon = 1e-10\n",
        "# lambda_params = 0.1\n",
        "# log_of_each_predictions_result = np.maximum(log_of_each_predictions, epsilon)\n",
        "\n",
        "\n",
        "# dot_product_of_log_predicted_and_predicted = np.dot(predictions_unlabeled_y.T, log_of_each_predictions_result)\n",
        "# print('kafkjhdahfjhfjkahfja', log_of_each_predictions_result.shape, predictions_unlabeled_y.shape, dot_product_of_log_predicted_and_predicted.shape)\n",
        "# result_unlabelled = np.sum(dot_product_of_log_predicted_and_predicted, axis=1)\n",
        "# print('Final result for unlabeled', result_unlabelled*lambda_params)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slig_rjjzWx-",
        "outputId": "dc504206-0aec-4476-b036-1e41b4c00c59"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "INSIDE ENTROPY MINIMIZATION LOSS Tensor(\"IteratorGetNext:1\", shape=(None, 10), dtype=float32) Tensor(\"sequential_15/dense_31/Softmax:0\", shape=(None, 10), dtype=float32)\n",
            "(None,)\n",
            "afdkafjk Tensor(\"entropy_minimization_loss/Sum:0\", shape=(None,), dtype=float32)\n",
            "INSIDE ENTROPY MINIMIZATION LOSS Tensor(\"IteratorGetNext:1\", shape=(None, 10), dtype=float32) Tensor(\"sequential_15/dense_31/Softmax:0\", shape=(None, 10), dtype=float32)\n",
            "(None,)\n",
            "afdkafjk Tensor(\"entropy_minimization_loss/Sum:0\", shape=(None,), dtype=float32)\n",
            "2/2 [==============================] - 1s 19ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 2/20\n",
            "2/2 [==============================] - 0s 20ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 3/20\n",
            "2/2 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 4/20\n",
            "2/2 [==============================] - 0s 23ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 5/20\n",
            "2/2 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 6/20\n",
            "2/2 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 7/20\n",
            "2/2 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 8/20\n",
            "2/2 [==============================] - 0s 17ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 9/20\n",
            "2/2 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 10/20\n",
            "2/2 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 11/20\n",
            "2/2 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 12/20\n",
            "2/2 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 13/20\n",
            "2/2 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 14/20\n",
            "2/2 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 15/20\n",
            "2/2 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 16/20\n",
            "2/2 [==============================] - 0s 21ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 17/20\n",
            "2/2 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 18/20\n",
            "2/2 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 19/20\n",
            "2/2 [==============================] - 0s 17ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 20/20\n",
            "2/2 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.1000\n",
            "INSIDE ENTROPY MINIMIZATION LOSS Tensor(\"IteratorGetNext:1\", shape=(None, 10), dtype=float32) Tensor(\"sequential_15/dense_31/Softmax:0\", shape=(None, 10), dtype=float32)\n",
            "(None,)\n",
            "afdkafjk Tensor(\"entropy_minimization_loss/Sum:0\", shape=(None,), dtype=float32)\n",
            "1/1 [==============================] - 0s 252ms/step - loss: nan - accuracy: 0.1000\n",
            "Test accuracy for entropy minimization 0.10000000149011612\n",
            "NaNs in input data: False\n",
            "Infinities in input data: False\n",
            "NaNs in labels: False\n",
            "Infinities in labels: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pseudo labels:\n",
        "pseudo_label_predictions = model.predict(unlabeled_dataset_x)\n",
        "max_predictions_value = np.argmax(pseudo_label_predictions)\n",
        "\n",
        "\n",
        "\n",
        "train_labels_y_value = keras.utils.to_categorical(labeled_dataset_to_be_trained_y, num_classes=10)\n",
        "#  combine labeled & unlabeled dataset:\n",
        "new_train_x = np.vstack((labeled_dataset_to_be_trained_x, unlabeled_dataset_x))\n",
        "new_train_y = np.vstack((train_labels_y_value, pseudo_label_predictions))\n",
        "\n",
        "\n",
        "print(labeled_dataset_to_be_trained_x.shape, unlabeled_dataset_x.shape, new_train_x.shape)\n",
        "print(train_labels_y.shape, pseudo_label_predictions.shape,new_train_y.shape)\n",
        "\n",
        "# train the model with this dataset\n",
        "labeled_x_train_subset_pseudo,labeled_x_test_subset_pseudo, labeled_y_train_subset_pseudo, labeled_y_test_subset_pseudo = train_test_split(\n",
        "    new_train_x, new_train_y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# train_labels_y_pseudo = keras.utils.to_categorical(labeled_y_train_subset_pseudo, num_classes=10)\n",
        "# test_labels_y_pseudo = keras.utils.to_categorical(labeled_y_test_subset_pseudo, num_classes=10)\n",
        "\n",
        "# Extract true labels for the labeled subset\n",
        "train_labels_y_pseudo = labeled_y_train_subset_pseudo[:, :10]\n",
        "test_labels_y_pseudo = labeled_y_test_subset_pseudo[:, :10]\n",
        "\n",
        "print(train_labels_y_pseudo.shape, test_labels_y_pseudo.shape, labeled_x_train_subset_pseudo.shape)\n",
        "# Train the model\n",
        "model.fit(labeled_x_train_subset_pseudo, train_labels_y_pseudo , epochs=20, batch_size=64)\n",
        "# model.fit(labeled_x_train_subset_pseudo, train_labels_y_pseudo , epochs=20, batch_size=64, validation_data=(labeled_x_test_subset_pseudo, test_labels_y_pseudo))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(labeled_x_test_subset_pseudo, test_labels_y_pseudo)\n",
        "print('\\nTest accuracy:', test_acc)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f06GRcr_LYA",
        "outputId": "ed1eca4e-28ac-45f8-a756-f182143596a5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "310/310 [==============================] - 1s 3ms/step\n",
            "98969 fjahjfhdalfj\n",
            "(9900, 10) 98969\n",
            "(100, 28, 28) (9900, 28, 28) (10000, 28, 28)\n",
            "(80, 10) (9900, 10) (10000, 10)\n",
            "(8000, 10) (2000, 10) (8000, 28, 28)\n",
            "Epoch 1/20\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 5.7006 - accuracy: 0.6386\n",
            "Epoch 2/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 1.6521 - accuracy: 0.5790\n",
            "Epoch 3/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 1.3484 - accuracy: 0.6061\n",
            "Epoch 4/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 1.2512 - accuracy: 0.6314\n",
            "Epoch 5/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 1.1232 - accuracy: 0.6411\n",
            "Epoch 6/20\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.0580 - accuracy: 0.6706\n",
            "Epoch 7/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 1.0285 - accuracy: 0.6726\n",
            "Epoch 8/20\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.0239 - accuracy: 0.6756\n",
            "Epoch 9/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.9614 - accuracy: 0.6865\n",
            "Epoch 10/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.9215 - accuracy: 0.7081\n",
            "Epoch 11/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.8959 - accuracy: 0.7084\n",
            "Epoch 12/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.8993 - accuracy: 0.7010\n",
            "Epoch 13/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.8831 - accuracy: 0.7171\n",
            "Epoch 14/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.8703 - accuracy: 0.7199\n",
            "Epoch 15/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.8389 - accuracy: 0.7232\n",
            "Epoch 16/20\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.8322 - accuracy: 0.7271\n",
            "Epoch 17/20\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.8320 - accuracy: 0.7369\n",
            "Epoch 18/20\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.8197 - accuracy: 0.7346\n",
            "Epoch 19/20\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.8214 - accuracy: 0.7381\n",
            "Epoch 20/20\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.8086 - accuracy: 0.7513\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.8035 - accuracy: 0.8040\n",
            "\n",
            "Test accuracy: 0.8040000200271606\n"
          ]
        }
      ]
    }
  ]
}