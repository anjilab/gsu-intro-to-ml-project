{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa0e24d7-27ec-40b0-8b1e-1bff80e5848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1a85e49-e21a-4b4c-bd8c-78e71f01b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries and functions to load the data\n",
    "from digits import get_mnist\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler, Subset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a91fd081-aa60-4252-bdd8-f6cdd7275e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47cfa6c5-c968-4a2f-8359-c161215f46c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels are [0 1 2 3 4 5 6 7 8 9]\n",
      "Labels are [0 1 2 3 4 5 6 7 8 9] 10000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 16  38  43  52  61  75  86 102 110 113]\n",
      "[46 49 54 65 67 78 80 81 88 89]\n",
      "[ 0  4  7  9 11 22 23 26 45 51]\n",
      "[ 6  8 10 30 35 40 57 63 87 95]\n",
      "[  5  28  48  55  56  71  98 106 120 128]\n",
      "[ 13  17  37  39  41  44  50  62  68 107]\n",
      "[14 25 53 69 72 73 82 83 91 92]\n",
      "[12 21 24 36 42 59 70 76 84 90]\n",
      "[ 1  3 15 18 20 31 33 47 60 64]\n",
      "[  2  19  27  29  32  34  58 121 137 139]\n",
      "afhdkjafhkjd 10000\n",
      "2\n",
      "155\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "desired_num_samples = 10000\n",
    "examples_per_class = 1000\n",
    "\n",
    "selected_indices = []\n",
    "\n",
    "# Iterate through each class to select the desired number of examples\n",
    "for class_idx in range(10):  # MNIST has 10 classes (digits 0 to 9)\n",
    "    class_indices = [idx for idx, label in enumerate(training_data.targets) if label == class_idx]\n",
    "    selected_indices.extend(class_indices[:examples_per_class])\n",
    "\n",
    "# Create a SubsetRandomSampler with the selected indices\n",
    "subset_sampler = SubsetRandomSampler(selected_indices)\n",
    "# Create data loaders.\n",
    "# train_dataloader = DataLoader(training_data, batch_size=batch_size, sampler=subset_sampler)\n",
    "# test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(training_data, sampler=subset_sampler)\n",
    "test_dataloader = DataLoader(test_data)\n",
    "\n",
    "X_input_image_arr = [X for batch, (X,y) in enumerate(train_dataloader)]\n",
    "Y_label_arr = [y for batch, (X,y) in enumerate(train_dataloader)]\n",
    "\n",
    "\n",
    "total_number_of_samples = len(train_dataloader)\n",
    "labeled_sample_for_every_class = 10\n",
    "\n",
    "unique_labels = np.unique([y for batch, (X,y) in enumerate(train_dataloader)])\n",
    "print(\"Labels are\", unique_labels)\n",
    "\n",
    "labeled_mask = np.zeros(total_number_of_samples, dtype=bool)\n",
    "\n",
    "\n",
    "print(\"Labels are\", np.unique([y for batch, (X,y) in enumerate(train_dataloader)]), len(labeled_mask))\n",
    "\n",
    "\n",
    "img = X_input_image_arr[0].numpy().squeeze()\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "for i in range(len(unique_labels)):\n",
    "    indices = np.where(np.array(Y_label_arr) == i)[0]\n",
    "    print(indices[:labeled_sample_for_every_class])\n",
    "    np.random.shuffle(indices)\n",
    "    labeled_mask[indices[:labeled_sample_for_every_class]] = True\n",
    "\n",
    "print(\"afhdkjafhkjd\",len(labeled_mask))\n",
    "\n",
    "labeled_dataset = Subset(train_dataloader, np.where(labeled_mask)[0])\n",
    "unlabeled_dataset = Subset(train_dataloader, np.where(~labeled_mask)[0])\n",
    "\n",
    "\n",
    "\n",
    "# for X, y in test_dataloader:\n",
    "#     print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "#     print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "#     break\n",
    "\n",
    "\n",
    "# for X, y in train_dataloader:\n",
    "#     print(f\"Shape of X [N, C, H, W]: {len(X)}\")\n",
    "#     print(f\"Shape of y: {len(y)} {y.dtype}\")\n",
    "#     break\n",
    "\n",
    "\n",
    "# DataLoader for labeled data\n",
    "labeled_dataloader =DataLoader(labeled_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# DataLoader for unlabeled data\n",
    "unlabeled_dataloader =DataLoader(unlabeled_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(len(labeled_dataloader))\n",
    "print(len(unlabeled_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e98486e-3ce7-4257-bf60-6fb285e8a3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=200, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=200, out_features=10, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 10),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "# model = NeuralNetwork().to(device)\n",
    "model = NeuralNetwork()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c32fe53a-55b3-41fa-a206-07b50a506576",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d4b1911-17d7-4553-a00b-399838c57774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_torch(dataloader, model, loss_fn, optimizer):\n",
    "    # size = len(dataloader.dataset)\n",
    "    size = len(dataloader)\n",
    "    \n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # X, y = X.to(device), y.to(device)\n",
    "        X, y = X, y\n",
    "        \n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1f97a1c-47d6-4bae-87c2-80679167bc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_torch(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            # X, y = X.to(device), y.to(device)\n",
    "            X, y = X, y\n",
    "            \n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e68fde5d-f2d9-449f-afbe-0ee61264953b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.328464  [    1/10000]\n",
      "loss: 2.326557  [  101/10000]\n",
      "loss: 2.312312  [  201/10000]\n",
      "loss: 2.319765  [  301/10000]\n",
      "loss: 2.340533  [  401/10000]\n",
      "loss: 2.259098  [  501/10000]\n",
      "loss: 2.327646  [  601/10000]\n",
      "loss: 2.256829  [  701/10000]\n",
      "loss: 2.344856  [  801/10000]\n",
      "loss: 2.325008  [  901/10000]\n",
      "loss: 2.342082  [ 1001/10000]\n",
      "loss: 2.115090  [ 1101/10000]\n",
      "loss: 1.481675  [ 1201/10000]\n",
      "loss: 2.413842  [ 1301/10000]\n",
      "loss: 1.908122  [ 1401/10000]\n",
      "loss: 2.219155  [ 1501/10000]\n",
      "loss: 2.427367  [ 1601/10000]\n",
      "loss: 2.255214  [ 1701/10000]\n",
      "loss: 1.961324  [ 1801/10000]\n",
      "loss: 2.444638  [ 1901/10000]\n",
      "loss: 2.392037  [ 2001/10000]\n",
      "loss: 2.132643  [ 2101/10000]\n",
      "loss: 1.805697  [ 2201/10000]\n",
      "loss: 2.040468  [ 2301/10000]\n",
      "loss: 2.166115  [ 2401/10000]\n",
      "loss: 2.326885  [ 2501/10000]\n",
      "loss: 2.121023  [ 2601/10000]\n",
      "loss: 1.986295  [ 2701/10000]\n",
      "loss: 2.394225  [ 2801/10000]\n",
      "loss: 0.477229  [ 2901/10000]\n",
      "loss: 1.548893  [ 3001/10000]\n",
      "loss: 2.470343  [ 3101/10000]\n",
      "loss: 1.471659  [ 3201/10000]\n",
      "loss: 0.322078  [ 3301/10000]\n",
      "loss: 1.197762  [ 3401/10000]\n",
      "loss: 2.480568  [ 3501/10000]\n",
      "loss: 2.496665  [ 3601/10000]\n",
      "loss: 1.407650  [ 3701/10000]\n",
      "loss: 1.522330  [ 3801/10000]\n",
      "loss: 1.884893  [ 3901/10000]\n",
      "loss: 1.377650  [ 4001/10000]\n",
      "loss: 1.480789  [ 4101/10000]\n",
      "loss: 1.234996  [ 4201/10000]\n",
      "loss: 0.477739  [ 4301/10000]\n",
      "loss: 2.535658  [ 4401/10000]\n",
      "loss: 1.733119  [ 4501/10000]\n",
      "loss: 1.010034  [ 4601/10000]\n",
      "loss: 1.010048  [ 4701/10000]\n",
      "loss: 1.611476  [ 4801/10000]\n",
      "loss: 1.643291  [ 4901/10000]\n",
      "loss: 0.679187  [ 5001/10000]\n",
      "loss: 0.575674  [ 5101/10000]\n",
      "loss: 0.942773  [ 5201/10000]\n",
      "loss: 2.794698  [ 5301/10000]\n",
      "loss: 1.348377  [ 5401/10000]\n",
      "loss: 1.813601  [ 5501/10000]\n",
      "loss: 1.325696  [ 5601/10000]\n",
      "loss: 1.236997  [ 5701/10000]\n",
      "loss: 1.104055  [ 5801/10000]\n",
      "loss: 1.194799  [ 5901/10000]\n",
      "loss: 1.029681  [ 6001/10000]\n",
      "loss: 0.920390  [ 6101/10000]\n",
      "loss: 1.087300  [ 6201/10000]\n",
      "loss: 1.617224  [ 6301/10000]\n",
      "loss: 2.311013  [ 6401/10000]\n",
      "loss: 2.798860  [ 6501/10000]\n",
      "loss: 0.486768  [ 6601/10000]\n",
      "loss: 0.526151  [ 6701/10000]\n",
      "loss: 0.935809  [ 6801/10000]\n",
      "loss: 0.571646  [ 6901/10000]\n",
      "loss: 0.723416  [ 7001/10000]\n",
      "loss: 2.610552  [ 7101/10000]\n",
      "loss: 0.189865  [ 7201/10000]\n",
      "loss: 1.857586  [ 7301/10000]\n",
      "loss: 0.576658  [ 7401/10000]\n",
      "loss: 0.599094  [ 7501/10000]\n",
      "loss: 0.813058  [ 7601/10000]\n",
      "loss: 0.881396  [ 7701/10000]\n",
      "loss: 1.328766  [ 7801/10000]\n",
      "loss: 1.838073  [ 7901/10000]\n",
      "loss: 2.570046  [ 8001/10000]\n",
      "loss: 0.624123  [ 8101/10000]\n",
      "loss: 0.663792  [ 8201/10000]\n",
      "loss: 1.187231  [ 8301/10000]\n",
      "loss: 0.971434  [ 8401/10000]\n",
      "loss: 0.300531  [ 8501/10000]\n",
      "loss: 2.490770  [ 8601/10000]\n",
      "loss: 1.592094  [ 8701/10000]\n",
      "loss: 0.311409  [ 8801/10000]\n",
      "loss: 1.312188  [ 8901/10000]\n",
      "loss: 0.208305  [ 9001/10000]\n",
      "loss: 0.198553  [ 9101/10000]\n",
      "loss: 0.447043  [ 9201/10000]\n",
      "loss: 3.082964  [ 9301/10000]\n",
      "loss: 1.667761  [ 9401/10000]\n",
      "loss: 2.743869  [ 9501/10000]\n",
      "loss: 0.232848  [ 9601/10000]\n",
      "loss: 0.329621  [ 9701/10000]\n",
      "loss: 0.198383  [ 9801/10000]\n",
      "loss: 0.631884  [ 9901/10000]\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.931595 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.662107  [    1/10000]\n",
      "loss: 2.640566  [  101/10000]\n",
      "loss: 0.849469  [  201/10000]\n",
      "loss: 2.323919  [  301/10000]\n",
      "loss: 2.899742  [  401/10000]\n",
      "loss: 3.695403  [  501/10000]\n",
      "loss: 0.962430  [  601/10000]\n",
      "loss: 0.988678  [  701/10000]\n",
      "loss: 0.234159  [  801/10000]\n",
      "loss: 0.063294  [  901/10000]\n",
      "loss: 0.029535  [ 1001/10000]\n",
      "loss: 0.837089  [ 1101/10000]\n",
      "loss: 1.439538  [ 1201/10000]\n",
      "loss: 0.259290  [ 1301/10000]\n",
      "loss: 1.293573  [ 1401/10000]\n",
      "loss: 0.208343  [ 1501/10000]\n",
      "loss: 0.371785  [ 1601/10000]\n",
      "loss: 0.012935  [ 1701/10000]\n",
      "loss: 0.624007  [ 1801/10000]\n",
      "loss: 0.228380  [ 1901/10000]\n",
      "loss: 1.287735  [ 2001/10000]\n",
      "loss: 0.327421  [ 2101/10000]\n",
      "loss: 0.171527  [ 2201/10000]\n",
      "loss: 0.059388  [ 2301/10000]\n",
      "loss: 0.160358  [ 2401/10000]\n",
      "loss: 2.737527  [ 2501/10000]\n",
      "loss: 1.013950  [ 2601/10000]\n",
      "loss: 2.062736  [ 2701/10000]\n",
      "loss: 0.692346  [ 2801/10000]\n",
      "loss: 1.473842  [ 2901/10000]\n",
      "loss: 0.018455  [ 3001/10000]\n",
      "loss: 0.081077  [ 3101/10000]\n",
      "loss: 0.169555  [ 3201/10000]\n",
      "loss: 0.540922  [ 3301/10000]\n",
      "loss: 0.131876  [ 3401/10000]\n",
      "loss: 1.588934  [ 3501/10000]\n",
      "loss: 0.152803  [ 3601/10000]\n",
      "loss: 1.566461  [ 3701/10000]\n",
      "loss: 0.189390  [ 3801/10000]\n",
      "loss: 0.108241  [ 3901/10000]\n",
      "loss: 0.605225  [ 4001/10000]\n",
      "loss: 0.610968  [ 4101/10000]\n",
      "loss: 0.003688  [ 4201/10000]\n",
      "loss: 0.786345  [ 4301/10000]\n",
      "loss: 0.497588  [ 4401/10000]\n",
      "loss: 0.999335  [ 4501/10000]\n",
      "loss: 1.350660  [ 4601/10000]\n",
      "loss: 0.354537  [ 4701/10000]\n",
      "loss: 0.355661  [ 4801/10000]\n",
      "loss: 0.568457  [ 4901/10000]\n",
      "loss: 0.910287  [ 5001/10000]\n",
      "loss: 0.107335  [ 5101/10000]\n",
      "loss: 0.409589  [ 5201/10000]\n",
      "loss: 0.214228  [ 5301/10000]\n",
      "loss: 0.100048  [ 5401/10000]\n",
      "loss: 0.063005  [ 5501/10000]\n",
      "loss: 0.200801  [ 5601/10000]\n",
      "loss: 0.332558  [ 5701/10000]\n",
      "loss: 0.032841  [ 5801/10000]\n",
      "loss: 0.033025  [ 5901/10000]\n",
      "loss: 0.161503  [ 6001/10000]\n",
      "loss: 0.395113  [ 6101/10000]\n",
      "loss: 0.357945  [ 6201/10000]\n",
      "loss: 0.981752  [ 6301/10000]\n",
      "loss: 1.594312  [ 6401/10000]\n",
      "loss: 0.313176  [ 6501/10000]\n",
      "loss: 0.011934  [ 6601/10000]\n",
      "loss: 0.043540  [ 6701/10000]\n",
      "loss: 0.093902  [ 6801/10000]\n",
      "loss: 0.775906  [ 6901/10000]\n",
      "loss: 3.257696  [ 7001/10000]\n",
      "loss: 0.298133  [ 7101/10000]\n",
      "loss: 0.010989  [ 7201/10000]\n",
      "loss: 0.558136  [ 7301/10000]\n",
      "loss: 1.091866  [ 7401/10000]\n",
      "loss: 2.574125  [ 7501/10000]\n",
      "loss: 0.011712  [ 7601/10000]\n",
      "loss: 0.100555  [ 7701/10000]\n",
      "loss: 0.119986  [ 7801/10000]\n",
      "loss: 0.284673  [ 7901/10000]\n",
      "loss: 0.129985  [ 8001/10000]\n",
      "loss: 0.085148  [ 8101/10000]\n",
      "loss: 0.040650  [ 8201/10000]\n",
      "loss: 0.558958  [ 8301/10000]\n",
      "loss: 2.180566  [ 8401/10000]\n",
      "loss: 0.004011  [ 8501/10000]\n",
      "loss: 0.358034  [ 8601/10000]\n",
      "loss: 0.077734  [ 8701/10000]\n",
      "loss: 3.602726  [ 8801/10000]\n",
      "loss: 3.120356  [ 8901/10000]\n",
      "loss: 0.844877  [ 9001/10000]\n",
      "loss: 1.160470  [ 9101/10000]\n",
      "loss: 0.673041  [ 9201/10000]\n",
      "loss: 0.029403  [ 9301/10000]\n",
      "loss: 0.071656  [ 9401/10000]\n",
      "loss: 0.118969  [ 9501/10000]\n",
      "loss: 0.133985  [ 9601/10000]\n",
      "loss: 0.003752  [ 9701/10000]\n",
      "loss: 0.007870  [ 9801/10000]\n",
      "loss: 3.203654  [ 9901/10000]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.664064 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.357191  [    1/10000]\n",
      "loss: 0.148846  [  101/10000]\n",
      "loss: 0.295541  [  201/10000]\n",
      "loss: 0.015032  [  301/10000]\n",
      "loss: 0.279835  [  401/10000]\n",
      "loss: 0.189000  [  501/10000]\n",
      "loss: 0.250689  [  601/10000]\n",
      "loss: 0.663728  [  701/10000]\n",
      "loss: 0.057416  [  801/10000]\n",
      "loss: 0.318118  [  901/10000]\n",
      "loss: 1.449114  [ 1001/10000]\n",
      "loss: 0.082007  [ 1101/10000]\n",
      "loss: 0.069751  [ 1201/10000]\n",
      "loss: 0.037328  [ 1301/10000]\n",
      "loss: 0.011016  [ 1401/10000]\n",
      "loss: 3.338335  [ 1501/10000]\n",
      "loss: 0.226525  [ 1601/10000]\n",
      "loss: 0.905865  [ 1701/10000]\n",
      "loss: 0.079898  [ 1801/10000]\n",
      "loss: 0.138313  [ 1901/10000]\n",
      "loss: 0.809409  [ 2001/10000]\n",
      "loss: 0.071007  [ 2101/10000]\n",
      "loss: 0.019498  [ 2201/10000]\n",
      "loss: 0.016796  [ 2301/10000]\n",
      "loss: 0.015627  [ 2401/10000]\n",
      "loss: 0.122915  [ 2501/10000]\n",
      "loss: 0.154218  [ 2601/10000]\n",
      "loss: 0.946404  [ 2701/10000]\n",
      "loss: 0.007921  [ 2801/10000]\n",
      "loss: 0.072295  [ 2901/10000]\n",
      "loss: 3.778196  [ 3001/10000]\n",
      "loss: 0.158531  [ 3101/10000]\n",
      "loss: 2.298571  [ 3201/10000]\n",
      "loss: 0.219973  [ 3301/10000]\n",
      "loss: 1.156107  [ 3401/10000]\n",
      "loss: 1.535035  [ 3501/10000]\n",
      "loss: 0.136705  [ 3601/10000]\n",
      "loss: 3.472921  [ 3701/10000]\n",
      "loss: 0.105431  [ 3801/10000]\n",
      "loss: 0.743865  [ 3901/10000]\n",
      "loss: 6.734015  [ 4001/10000]\n",
      "loss: 0.029860  [ 4101/10000]\n",
      "loss: 2.679460  [ 4201/10000]\n",
      "loss: 0.538441  [ 4301/10000]\n",
      "loss: 0.033873  [ 4401/10000]\n",
      "loss: 0.058783  [ 4501/10000]\n",
      "loss: 0.253359  [ 4601/10000]\n",
      "loss: 0.028145  [ 4701/10000]\n",
      "loss: 0.093946  [ 4801/10000]\n",
      "loss: 0.072116  [ 4901/10000]\n",
      "loss: 3.674401  [ 5001/10000]\n",
      "loss: 0.079412  [ 5101/10000]\n",
      "loss: 0.016553  [ 5201/10000]\n",
      "loss: 0.837076  [ 5301/10000]\n",
      "loss: 0.003844  [ 5401/10000]\n",
      "loss: 2.698141  [ 5501/10000]\n",
      "loss: 0.020308  [ 5601/10000]\n",
      "loss: 0.362303  [ 5701/10000]\n",
      "loss: 0.617700  [ 5801/10000]\n",
      "loss: 0.027389  [ 5901/10000]\n",
      "loss: 0.032688  [ 6001/10000]\n",
      "loss: 2.007302  [ 6101/10000]\n",
      "loss: 2.302585  [ 6201/10000]\n",
      "loss: 0.020343  [ 6301/10000]\n",
      "loss: 0.034845  [ 6401/10000]\n",
      "loss: 2.667686  [ 6501/10000]\n",
      "loss: 1.978323  [ 6601/10000]\n",
      "loss: 0.000466  [ 6701/10000]\n",
      "loss: 1.251461  [ 6801/10000]\n",
      "loss: 0.105751  [ 6901/10000]\n",
      "loss: 0.968610  [ 7001/10000]\n",
      "loss: 0.127433  [ 7101/10000]\n",
      "loss: 0.078339  [ 7201/10000]\n",
      "loss: 0.202984  [ 7301/10000]\n",
      "loss: 0.348986  [ 7401/10000]\n",
      "loss: 0.256848  [ 7501/10000]\n",
      "loss: 0.031707  [ 7601/10000]\n",
      "loss: 2.562987  [ 7701/10000]\n",
      "loss: 0.356225  [ 7801/10000]\n",
      "loss: 2.429745  [ 7901/10000]\n",
      "loss: 0.034948  [ 8001/10000]\n",
      "loss: 0.569023  [ 8101/10000]\n",
      "loss: 0.001217  [ 8201/10000]\n",
      "loss: 0.004272  [ 8301/10000]\n",
      "loss: 0.177241  [ 8401/10000]\n",
      "loss: 0.586927  [ 8501/10000]\n",
      "loss: 2.395791  [ 8601/10000]\n",
      "loss: 0.087064  [ 8701/10000]\n",
      "loss: 0.063784  [ 8801/10000]\n",
      "loss: 0.612352  [ 8901/10000]\n",
      "loss: 0.139161  [ 9001/10000]\n",
      "loss: 2.847950  [ 9101/10000]\n",
      "loss: 0.021964  [ 9201/10000]\n",
      "loss: 0.046600  [ 9301/10000]\n",
      "loss: 1.057393  [ 9401/10000]\n",
      "loss: 0.052700  [ 9501/10000]\n",
      "loss: 0.034711  [ 9601/10000]\n",
      "loss: 0.027685  [ 9701/10000]\n",
      "loss: 0.290500  [ 9801/10000]\n",
      "loss: 0.055112  [ 9901/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.590359 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.037394  [    1/10000]\n",
      "loss: 0.004280  [  101/10000]\n",
      "loss: 0.006572  [  201/10000]\n",
      "loss: 0.235304  [  301/10000]\n",
      "loss: 0.088324  [  401/10000]\n",
      "loss: 0.119116  [  501/10000]\n",
      "loss: 0.039827  [  601/10000]\n",
      "loss: 3.609866  [  701/10000]\n",
      "loss: 0.013269  [  801/10000]\n",
      "loss: 0.001069  [  901/10000]\n",
      "loss: 0.119109  [ 1001/10000]\n",
      "loss: 0.003149  [ 1101/10000]\n",
      "loss: 2.486361  [ 1201/10000]\n",
      "loss: 0.045839  [ 1301/10000]\n",
      "loss: 0.744234  [ 1401/10000]\n",
      "loss: 0.077182  [ 1501/10000]\n",
      "loss: 2.302585  [ 1601/10000]\n",
      "loss: 0.179415  [ 1701/10000]\n",
      "loss: 0.002430  [ 1801/10000]\n",
      "loss: 0.606176  [ 1901/10000]\n",
      "loss: 0.065178  [ 2001/10000]\n",
      "loss: 0.980520  [ 2101/10000]\n",
      "loss: 0.459511  [ 2201/10000]\n",
      "loss: 0.730097  [ 2301/10000]\n",
      "loss: 1.344237  [ 2401/10000]\n",
      "loss: 0.012953  [ 2501/10000]\n",
      "loss: 0.016359  [ 2601/10000]\n",
      "loss: 0.004141  [ 2701/10000]\n",
      "loss: 0.546655  [ 2801/10000]\n",
      "loss: 0.102346  [ 2901/10000]\n",
      "loss: 0.086692  [ 3001/10000]\n",
      "loss: 0.000482  [ 3101/10000]\n",
      "loss: 0.010706  [ 3201/10000]\n",
      "loss: 0.540055  [ 3301/10000]\n",
      "loss: 0.297477  [ 3401/10000]\n",
      "loss: 1.406216  [ 3501/10000]\n",
      "loss: 0.728244  [ 3601/10000]\n",
      "loss: 0.391973  [ 3701/10000]\n",
      "loss: 0.010595  [ 3801/10000]\n",
      "loss: 0.004958  [ 3901/10000]\n",
      "loss: 0.303291  [ 4001/10000]\n",
      "loss: 0.194852  [ 4101/10000]\n",
      "loss: 0.072952  [ 4201/10000]\n",
      "loss: 0.326798  [ 4301/10000]\n",
      "loss: 0.039792  [ 4401/10000]\n",
      "loss: 2.116735  [ 4501/10000]\n",
      "loss: 0.412073  [ 4601/10000]\n",
      "loss: 0.005813  [ 4701/10000]\n",
      "loss: 0.000233  [ 4801/10000]\n",
      "loss: 0.134673  [ 4901/10000]\n",
      "loss: 0.070192  [ 5001/10000]\n",
      "loss: 0.091747  [ 5101/10000]\n",
      "loss: 0.000842  [ 5201/10000]\n",
      "loss: 0.080731  [ 5301/10000]\n",
      "loss: 0.206722  [ 5401/10000]\n",
      "loss: 0.018855  [ 5501/10000]\n",
      "loss: 0.148101  [ 5601/10000]\n",
      "loss: 0.023290  [ 5701/10000]\n",
      "loss: 4.086707  [ 5801/10000]\n",
      "loss: 0.017348  [ 5901/10000]\n",
      "loss: 3.902781  [ 6001/10000]\n",
      "loss: 0.027083  [ 6101/10000]\n",
      "loss: 3.847658  [ 6201/10000]\n",
      "loss: 0.000500  [ 6301/10000]\n",
      "loss: 0.016553  [ 6401/10000]\n",
      "loss: 0.027795  [ 6501/10000]\n",
      "loss: 0.005767  [ 6601/10000]\n",
      "loss: 0.199807  [ 6701/10000]\n",
      "loss: 0.231424  [ 6801/10000]\n",
      "loss: 0.069261  [ 6901/10000]\n",
      "loss: 0.050259  [ 7001/10000]\n",
      "loss: 0.016825  [ 7101/10000]\n",
      "loss: 0.404709  [ 7201/10000]\n",
      "loss: 0.158460  [ 7301/10000]\n",
      "loss: 0.018465  [ 7401/10000]\n",
      "loss: 0.182809  [ 7501/10000]\n",
      "loss: 0.013544  [ 7601/10000]\n",
      "loss: 0.098976  [ 7701/10000]\n",
      "loss: 0.086500  [ 7801/10000]\n",
      "loss: 1.314060  [ 7901/10000]\n",
      "loss: 0.001846  [ 8001/10000]\n",
      "loss: 0.005476  [ 8101/10000]\n",
      "loss: 0.000258  [ 8201/10000]\n",
      "loss: 0.017034  [ 8301/10000]\n",
      "loss: 3.985642  [ 8401/10000]\n",
      "loss: 0.158085  [ 8501/10000]\n",
      "loss: 0.050546  [ 8601/10000]\n",
      "loss: 0.007308  [ 8701/10000]\n",
      "loss: 0.010697  [ 8801/10000]\n",
      "loss: 0.029455  [ 8901/10000]\n",
      "loss: 0.017296  [ 9001/10000]\n",
      "loss: 0.009927  [ 9101/10000]\n",
      "loss: 0.021340  [ 9201/10000]\n",
      "loss: 0.037609  [ 9301/10000]\n",
      "loss: 0.092048  [ 9401/10000]\n",
      "loss: 0.775799  [ 9501/10000]\n",
      "loss: 0.013283  [ 9601/10000]\n",
      "loss: 0.007187  [ 9701/10000]\n",
      "loss: 2.331753  [ 9801/10000]\n",
      "loss: 0.733701  [ 9901/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.549072 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.302585  [    1/10000]\n",
      "loss: 0.031641  [  101/10000]\n",
      "loss: 2.725481  [  201/10000]\n",
      "loss: 2.616816  [  301/10000]\n",
      "loss: 0.031690  [  401/10000]\n",
      "loss: 0.011976  [  501/10000]\n",
      "loss: 0.122650  [  601/10000]\n",
      "loss: 0.016274  [  701/10000]\n",
      "loss: 0.067965  [  801/10000]\n",
      "loss: 4.019828  [  901/10000]\n",
      "loss: 0.017306  [ 1001/10000]\n",
      "loss: 0.156817  [ 1101/10000]\n",
      "loss: 0.088809  [ 1201/10000]\n",
      "loss: 0.796723  [ 1301/10000]\n",
      "loss: 0.054993  [ 1401/10000]\n",
      "loss: 1.950898  [ 1501/10000]\n",
      "loss: 0.489806  [ 1601/10000]\n",
      "loss: 2.346609  [ 1701/10000]\n",
      "loss: 0.065752  [ 1801/10000]\n",
      "loss: 0.017083  [ 1901/10000]\n",
      "loss: 1.569592  [ 2001/10000]\n",
      "loss: 0.008030  [ 2101/10000]\n",
      "loss: 0.058299  [ 2201/10000]\n",
      "loss: 0.000540  [ 2301/10000]\n",
      "loss: 2.711780  [ 2401/10000]\n",
      "loss: 0.030378  [ 2501/10000]\n",
      "loss: 0.433677  [ 2601/10000]\n",
      "loss: 0.188805  [ 2701/10000]\n",
      "loss: 0.008108  [ 2801/10000]\n",
      "loss: 0.092113  [ 2901/10000]\n",
      "loss: 0.036950  [ 3001/10000]\n",
      "loss: 1.196183  [ 3101/10000]\n",
      "loss: 0.006324  [ 3201/10000]\n",
      "loss: 0.122999  [ 3301/10000]\n",
      "loss: 0.004006  [ 3401/10000]\n",
      "loss: 0.044593  [ 3501/10000]\n",
      "loss: 0.122346  [ 3601/10000]\n",
      "loss: 0.003136  [ 3701/10000]\n",
      "loss: 0.206136  [ 3801/10000]\n",
      "loss: 0.018548  [ 3901/10000]\n",
      "loss: 0.070613  [ 4001/10000]\n",
      "loss: 3.579453  [ 4101/10000]\n",
      "loss: 0.004480  [ 4201/10000]\n",
      "loss: 0.116902  [ 4301/10000]\n",
      "loss: 0.081644  [ 4401/10000]\n",
      "loss: 0.424791  [ 4501/10000]\n",
      "loss: 0.061495  [ 4601/10000]\n",
      "loss: 0.010600  [ 4701/10000]\n",
      "loss: 0.036467  [ 4801/10000]\n",
      "loss: 0.833924  [ 4901/10000]\n",
      "loss: 0.428945  [ 5001/10000]\n",
      "loss: 1.594983  [ 5101/10000]\n",
      "loss: 0.304664  [ 5201/10000]\n",
      "loss: 0.120803  [ 5301/10000]\n",
      "loss: 1.113526  [ 5401/10000]\n",
      "loss: 0.777644  [ 5501/10000]\n",
      "loss: 0.327401  [ 5601/10000]\n",
      "loss: 4.011005  [ 5701/10000]\n",
      "loss: 0.527379  [ 5801/10000]\n",
      "loss: 0.026727  [ 5901/10000]\n",
      "loss: 0.500839  [ 6001/10000]\n",
      "loss: 0.005276  [ 6101/10000]\n",
      "loss: 0.305133  [ 6201/10000]\n",
      "loss: 0.057201  [ 6301/10000]\n",
      "loss: 2.456324  [ 6401/10000]\n",
      "loss: 0.028408  [ 6501/10000]\n",
      "loss: 0.016185  [ 6601/10000]\n",
      "loss: 0.269176  [ 6701/10000]\n",
      "loss: 0.144867  [ 6801/10000]\n",
      "loss: 2.078850  [ 6901/10000]\n",
      "loss: 2.317088  [ 7001/10000]\n",
      "loss: 0.011743  [ 7101/10000]\n",
      "loss: 0.240086  [ 7201/10000]\n",
      "loss: 2.353931  [ 7301/10000]\n",
      "loss: 2.533340  [ 7401/10000]\n",
      "loss: 0.010062  [ 7501/10000]\n",
      "loss: 2.394152  [ 7601/10000]\n",
      "loss: 0.001106  [ 7701/10000]\n",
      "loss: 0.005476  [ 7801/10000]\n",
      "loss: 0.032997  [ 7901/10000]\n",
      "loss: 0.107080  [ 8001/10000]\n",
      "loss: 0.015210  [ 8101/10000]\n",
      "loss: 0.721087  [ 8201/10000]\n",
      "loss: 0.000063  [ 8301/10000]\n",
      "loss: 0.137439  [ 8401/10000]\n",
      "loss: 2.445980  [ 8501/10000]\n",
      "loss: 0.928612  [ 8601/10000]\n",
      "loss: 0.512182  [ 8701/10000]\n",
      "loss: 0.460414  [ 8801/10000]\n",
      "loss: 0.301070  [ 8901/10000]\n",
      "loss: 0.073755  [ 9001/10000]\n",
      "loss: 0.057989  [ 9101/10000]\n",
      "loss: 0.274903  [ 9201/10000]\n",
      "loss: 0.064035  [ 9301/10000]\n",
      "loss: 2.441565  [ 9401/10000]\n",
      "loss: 0.058433  [ 9501/10000]\n",
      "loss: 0.045338  [ 9601/10000]\n",
      "loss: 0.017161  [ 9701/10000]\n",
      "loss: 0.765567  [ 9801/10000]\n",
      "loss: 0.175200  [ 9901/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.538231 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# This can be uncommented when you need to torch\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_torch(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_torch(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ffbe78d-bad9-4fe8-98e1-42c13568e567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels [0 1 2 3 4 5 6 7 8 9]\n",
      "Category 0: 1000 samples\n",
      "Category 2: 1000 samples\n",
      "Category 4: 1000 samples\n",
      "Category 3: 1000 samples\n",
      "Category 6: 1000 samples\n",
      "Category 7: 1000 samples\n",
      "Category 9: 1000 samples\n",
      "Category 8: 1000 samples\n",
      "Category 5: 1000 samples\n",
      "Category 1: 1000 samples\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Randomly select samples from each category\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m unique_labels:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Find indices of samples with the current label\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(\u001b[43mlabels\u001b[49m \u001b[38;5;241m==\u001b[39m label)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# Randomly select 'samples_per_category' samples from the current category\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     selected_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(indices, size\u001b[38;5;241m=\u001b[39msamples_per_category, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming the last column contains category labels\n",
    "# training_data_labels = training_data[:, -1]\n",
    "\n",
    "# Get unique category labels\n",
    "unique_labels = np.unique([y for batch, (X,y) in enumerate(train_dataloader)])\n",
    "\n",
    "print(\"Unique labels\", unique_labels)\n",
    "# print(\n",
    "\n",
    "# Set the number of samples you want to select from each category\n",
    "samples_per_category = 10\n",
    "\n",
    "# Initialize an empty array to store selected samples\n",
    "# selected_samples = np.empty((0, data.shape[1]))\n",
    "\n",
    "\n",
    "def count_samples_per_category(dataloader):\n",
    "    category_counts = {}\n",
    "\n",
    "    for _, labels in dataloader:\n",
    "        for label in labels:\n",
    "            category_counts[label.item()] = category_counts.get(label.item(), 0) + 1\n",
    "\n",
    "    return category_counts\n",
    "\n",
    "\n",
    "\n",
    "# Count the total number of samples in each category\n",
    "category_counts = count_samples_per_category(train_dataloader)\n",
    "\n",
    "# Print the results\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"Category {category}: {count} samples\")\n",
    "\n",
    "\n",
    "\n",
    "# Randomly select samples from each category\n",
    "for label in unique_labels:\n",
    "    # Find indices of samples with the current label\n",
    "    indices = np.where(labels == label)[0]\n",
    "\n",
    "    # Randomly select 'samples_per_category' samples from the current category\n",
    "    selected_indices = np.random.choice(indices, size=samples_per_category, replace=False)\n",
    "\n",
    "    # Append selected samples to the result array\n",
    "    selected_samples = np.vstack((selected_samples, data[selected_indices]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
