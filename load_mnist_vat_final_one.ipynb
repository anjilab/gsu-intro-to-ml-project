{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.losses import KLDivergence, sparse_categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n"
      ],
      "metadata": {
        "id": "xGqzAgZwzE5S"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "ZEpiD4KQzAVW",
        "outputId": "2316db32-229b-4882-dc78-7b2400677566"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Combine training and test datasets to get the full dataset\n",
        "x_full = np.concatenate((x_train, x_test), axis=0)\n",
        "y_full = np.concatenate((y_train, y_test), axis=0)\n",
        "\n",
        "\n",
        "# Function to select a specified number of samples for each category\n",
        "def select_samples(x, y, num_samples):\n",
        "    selected_samples = []\n",
        "    selected_labels = []\n",
        "    for i in range(10):  # 10 categories in MNIST\n",
        "        indices = np.where(y == i)[0][:num_samples]\n",
        "        selected_samples.append(x[indices])\n",
        "        selected_labels.append(y[indices])\n",
        "    selected_samples = np.concatenate(selected_samples, axis=0)\n",
        "    selected_labels = np.concatenate(selected_labels, axis=0)\n",
        "    return selected_samples, selected_labels\n",
        "\n",
        "# Select 1,000 samples for each category\n",
        "x_subset, y_subset = select_samples(x_full, y_full, 1000)\n",
        "\n",
        "# Display one random image from x_train_subset\n",
        "index = np.random.randint(0, len(x_subset))\n",
        "image = x_subset[index]\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.title(f\"Label: {y_subset[index]}\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Split the subset into training and testing sets\n",
        "x_train_subset, x_test_subset, y_train_subset, y_test_subset = train_test_split(\n",
        "    x_subset, y_subset, test_size=0.2, random_state=42\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "labeled_mask = np.zeros(10000, dtype=bool)\n",
        "for i in range(10):\n",
        "    indices = np.where(np.array(y_subset) == i)[0] # returns shuffled array of indices of each class, total 1000 data.\n",
        "    np.random.shuffle(indices)\n",
        "    labeled_mask[indices[:10]] = True\n",
        "\n",
        "\n",
        "print(np.where(labeled_mask)[0])\n",
        "\n",
        "labeled_dataset_to_be_trained_index = np.where(labeled_mask)[0]\n",
        "unlabeled_dataset_index = np.where(~labeled_mask)[0]\n",
        "\n",
        "\n",
        "\n",
        "labeled_dataset_to_be_trained_x = np.array([x_subset[index]  for index in labeled_dataset_to_be_trained_index])\n",
        "labeled_dataset_to_be_trained_y = np.array([y_subset[index]  for index in labeled_dataset_to_be_trained_index])\n",
        "\n",
        "unlabeled_dataset_x = np.array([x_subset[index]  for index in unlabeled_dataset_index])\n",
        "# unlabeled_dataset_y = np.array([y_subset[index]  for index in unlabeled_dataset_index])\n",
        "\n",
        "print(len(labeled_dataset_to_be_trained_y))\n",
        "\n",
        "unique_labels = np.unique([y for y in labeled_dataset_to_be_trained_y])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Display one random image from x_train_subset\n",
        "index = np.random.randint(0, len(labeled_dataset_to_be_trained_x))\n",
        "image = labeled_dataset_to_be_trained_x[index]\n",
        "\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.title(f\"Label: {labeled_dataset_to_be_trained_y[index]}\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# train_labels_one_hot_y = keras.utils.to_categorical(unique_labels, num_classes=10)\n",
        "\n",
        "# Split the subset into training and testing sets\n",
        "labeled_x_train_subset,labeled_x_test_subset, labeled_y_train_subset, labeled_y_test_subset = train_test_split(\n",
        "    labeled_dataset_to_be_trained_x, labeled_dataset_to_be_trained_y, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "train_labels_y = keras.utils.to_categorical(labeled_y_train_subset, num_classes=10)\n",
        "test_labels_y = keras.utils.to_categorical(labeled_y_test_subset, num_classes=10)\n",
        "\n",
        "print(train_labels_y.shape, 'akfdkafdkjh', labeled_y_train_subset.shape, labeled_dataset_to_be_trained_y.shape)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "UsNNtoSwzzp0",
        "outputId": "3fdc0552-dccd-4a5d-affd-ff91406f23fb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  27  231  298  306  553  672  706  859  971  993 1018 1201 1276 1386\n",
            " 1506 1532 1601 1671 1845 1939 2070 2071 2181 2222 2489 2500 2676 2680\n",
            " 2842 2947 3228 3277 3325 3329 3350 3503 3582 3587 3625 3834 4027 4081\n",
            " 4399 4636 4727 4756 4796 4797 4820 4947 5222 5495 5604 5647 5653 5853\n",
            " 5859 5894 5935 5985 6050 6128 6273 6401 6420 6516 6538 6623 6757 6775\n",
            " 7331 7376 7384 7461 7522 7532 7625 7647 7799 7848 8027 8221 8224 8242\n",
            " 8385 8428 8762 8777 8854 8996 9009 9181 9292 9712 9719 9748 9804 9838\n",
            " 9985 9990]\n",
            "100\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(90, 10) akfdkafdkjh (90,) (100,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the neural network model\n",
        "model = keras.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28,1)),  # Assuming input images are 28x28\n",
        "    layers.Dense(200, activation='relu'),   # Hidden layer with 200 neurons and ReLU activation\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "# model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "# model.fit(labeled_x_train_subset, train_labels_y , epochs=20, batch_size=64, validation_data=(labeled_x_test_subset, test_labels_y))\n",
        "model.fit(labeled_x_train_subset, train_labels_y , epochs=200, batch_size=64)\n",
        "\n",
        "print(labeled_x_test_subset.shape)\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(labeled_x_test_subset, test_labels_y)\n",
        "# _, train_acc_acc = model.evaluate(labeled_x_train_subset, train_labels_y)\n",
        "\n",
        "print('\\nTest accuracy:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zMQPODd5fYK",
        "outputId": "bac707e5-bc88-47c6-a139-6885fd5b37d5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "2/2 [==============================] - 1s 13ms/step - loss: 211.0448 - accuracy: 0.0444\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 167.5184 - accuracy: 0.1333\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 109.6602 - accuracy: 0.3000\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 82.9648 - accuracy: 0.3556\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 44.2092 - accuracy: 0.5333\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 37.6659 - accuracy: 0.5444\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 24.1998 - accuracy: 0.5889\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 26.2756 - accuracy: 0.6889\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 15.8498 - accuracy: 0.7667\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 9.5554 - accuracy: 0.8333\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 9.0874 - accuracy: 0.7333\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 7.0027 - accuracy: 0.8667\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 7.8354 - accuracy: 0.8333\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 2.8783 - accuracy: 0.9333\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 2.6568 - accuracy: 0.9000\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 4.6318 - accuracy: 0.8778\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 4.1486 - accuracy: 0.8444\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 4.7270 - accuracy: 0.9111\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.8571 - accuracy: 0.9333\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 3.0486 - accuracy: 0.9333\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.7940 - accuracy: 0.9556\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.7997 - accuracy: 0.9222\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2.3478 - accuracy: 0.9222\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.6579 - accuracy: 0.9111\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.5214 - accuracy: 0.9111\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.9265 - accuracy: 0.8778\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1.5552 - accuracy: 0.9556\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.4526 - accuracy: 0.9333\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.9261 - accuracy: 0.9556\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.1358 - accuracy: 0.9444\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.6543 - accuracy: 0.9556\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.5708 - accuracy: 0.9778\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.8530 - accuracy: 0.9889\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.6204 - accuracy: 0.9556\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.6531 - accuracy: 0.9778\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1.1606 - accuracy: 0.9667\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.7429 - accuracy: 0.9778\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 2.2911 - accuracy: 0.9556\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.6328 - accuracy: 0.9778\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2337 - accuracy: 0.9778\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.5772 - accuracy: 0.9667\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.5770 - accuracy: 0.9667\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2097 - accuracy: 0.9667\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.6613 - accuracy: 0.9667\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.8008 - accuracy: 0.9778\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2679 - accuracy: 0.9778\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.8883 - accuracy: 0.9444\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0798 - accuracy: 0.9778\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.5291 - accuracy: 0.9667\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.5171 - accuracy: 0.9889\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3016 - accuracy: 0.9667\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2445 - accuracy: 0.9889\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.9031 - accuracy: 0.9667\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2024 - accuracy: 0.9889\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.6329 - accuracy: 0.9444\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.7317 - accuracy: 0.9889\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.4137 - accuracy: 0.9889\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0072 - accuracy: 1.0000\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4794 - accuracy: 0.9778\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.8362 - accuracy: 0.9778\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.8240 - accuracy: 0.9667\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4220 - accuracy: 0.9889\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3955 - accuracy: 0.9778\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.6586 - accuracy: 0.9667\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.4025 - accuracy: 0.9889\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4527 - accuracy: 0.9889\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.9026 - accuracy: 0.9667\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0615 - accuracy: 0.9889\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.9646 - accuracy: 0.9556\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3117 - accuracy: 0.9889\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.0435 - accuracy: 0.9556\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.6186e-04 - accuracy: 1.0000\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2853 - accuracy: 0.9889\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.4362 - accuracy: 0.9556\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.0161 - accuracy: 0.9667\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2917 - accuracy: 0.9778\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3221 - accuracy: 0.9778\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2207 - accuracy: 0.9667\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1156 - accuracy: 0.9889\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.7299 - accuracy: 0.9667\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2948 - accuracy: 0.9778\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.6579 - accuracy: 0.9778\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1112 - accuracy: 0.9889\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.5385 - accuracy: 0.9667\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.3412 - accuracy: 0.9667\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2709 - accuracy: 0.9889\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3409 - accuracy: 0.9889\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1068 - accuracy: 0.9889\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1714 - accuracy: 0.9889\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2313 - accuracy: 0.9889\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.7297 - accuracy: 0.9778\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0171 - accuracy: 0.9889\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.9660 - accuracy: 0.9667\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.2569 - accuracy: 0.9889\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.7168 - accuracy: 0.9778\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0242 - accuracy: 0.9889\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 6.6227e-08 - accuracy: 1.0000\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3311 - accuracy: 0.9889\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 4.3450e-06 - accuracy: 1.0000\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1042 - accuracy: 0.9889\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0542 - accuracy: 0.9889\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3296 - accuracy: 0.9778\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 6.6407e-06 - accuracy: 1.0000\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 8.6050e-05 - accuracy: 1.0000\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2546 - accuracy: 0.9889\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 3.7087e-08 - accuracy: 1.0000\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.3497 - accuracy: 0.9778\n",
            "Epoch 112/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 113/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 114/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4104 - accuracy: 0.9889\n",
            "Epoch 115/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1479 - accuracy: 0.9889\n",
            "Epoch 116/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 9.0069e-08 - accuracy: 1.0000\n",
            "Epoch 117/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0755 - accuracy: 0.9889\n",
            "Epoch 118/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1621 - accuracy: 0.9889\n",
            "Epoch 119/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1.3245e-09 - accuracy: 1.0000\n",
            "Epoch 120/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.6260e-04 - accuracy: 1.0000\n",
            "Epoch 121/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2495 - accuracy: 0.9667\n",
            "Epoch 122/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2962 - accuracy: 0.9778\n",
            "Epoch 123/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2518 - accuracy: 0.9778\n",
            "Epoch 124/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.4814 - accuracy: 0.9889\n",
            "Epoch 125/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2042 - accuracy: 0.9889\n",
            "Epoch 126/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3370 - accuracy: 0.9889\n",
            "Epoch 127/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.8794e-06 - accuracy: 1.0000\n",
            "Epoch 128/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1126 - accuracy: 0.9889\n",
            "Epoch 129/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 2.7403e-04 - accuracy: 1.0000\n",
            "Epoch 130/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.8087 - accuracy: 0.9556\n",
            "Epoch 131/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 6.8983e-04 - accuracy: 1.0000\n",
            "Epoch 132/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3390 - accuracy: 0.9778\n",
            "Epoch 133/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.3249 - accuracy: 0.9889\n",
            "Epoch 134/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 6.8477e-07 - accuracy: 1.0000\n",
            "Epoch 135/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0436 - accuracy: 0.9889\n",
            "Epoch 136/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4002 - accuracy: 0.9889\n",
            "Epoch 137/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0073 - accuracy: 1.0000\n",
            "Epoch 138/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1.1313 - accuracy: 0.9778\n",
            "Epoch 139/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 9.2847e-07 - accuracy: 1.0000\n",
            "Epoch 140/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 9.0069e-08 - accuracy: 1.0000\n",
            "Epoch 141/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 8.0265e-07 - accuracy: 1.0000\n",
            "Epoch 142/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.2021e-04 - accuracy: 1.0000\n",
            "Epoch 143/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 2.5431e-07 - accuracy: 1.0000\n",
            "Epoch 144/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1976 - accuracy: 0.9889\n",
            "Epoch 145/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0367 - accuracy: 0.9889\n",
            "Epoch 146/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.0773 - accuracy: 0.9778\n",
            "Epoch 147/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.3245e-08 - accuracy: 1.0000\n",
            "Epoch 148/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1493 - accuracy: 0.9889\n",
            "Epoch 149/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0239 - accuracy: 0.9889\n",
            "Epoch 150/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.6528 - accuracy: 0.9778\n",
            "Epoch 151/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 152/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 153/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.3312 - accuracy: 0.9667\n",
            "Epoch 154/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 155/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.3586 - accuracy: 0.9778\n",
            "Epoch 156/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2.6491e-09 - accuracy: 1.0000\n",
            "Epoch 157/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 3.1838e-06 - accuracy: 1.0000\n",
            "Epoch 158/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 159/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 7.3859e-06 - accuracy: 1.0000\n",
            "Epoch 160/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 4.7329e-06 - accuracy: 1.0000\n",
            "Epoch 161/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 162/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0784 - accuracy: 0.9889\n",
            "Epoch 163/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2.2120e-07 - accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.8906 - accuracy: 0.9667\n",
            "Epoch 165/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1128 - accuracy: 0.9889\n",
            "Epoch 166/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.8209e-04 - accuracy: 1.0000\n",
            "Epoch 167/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.7087e-08 - accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0180 - accuracy: 0.9889\n",
            "Epoch 169/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4646 - accuracy: 0.9889\n",
            "Epoch 170/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.6553 - accuracy: 0.9778\n",
            "Epoch 171/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.7524 - accuracy: 0.9778\n",
            "Epoch 172/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3347 - accuracy: 0.9778\n",
            "Epoch 173/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.2590 - accuracy: 0.9889\n",
            "Epoch 174/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 2.2740e-05 - accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2413 - accuracy: 0.9889\n",
            "Epoch 176/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0397 - accuracy: 0.9889\n",
            "Epoch 177/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 4.4636e-07 - accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0541 - accuracy: 0.9778\n",
            "Epoch 179/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3228 - accuracy: 0.9889\n",
            "Epoch 180/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0817 - accuracy: 0.9889\n",
            "Epoch 181/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0556 - accuracy: 0.9889\n",
            "Epoch 182/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.4668e-05 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 4.1193e-07 - accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.8351 - accuracy: 0.9667\n",
            "Epoch 186/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.5400 - accuracy: 0.9889\n",
            "Epoch 187/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 2.0102e-05 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.9332 - accuracy: 0.9889\n",
            "Epoch 189/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1.9590e-05 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 4.8966e-05 - accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 9.2718e-09 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0400 - accuracy: 0.9889\n",
            "Epoch 194/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0371 - accuracy: 0.9889\n",
            "Epoch 195/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.3740 - accuracy: 0.9889\n",
            "Epoch 196/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0215 - accuracy: 0.9889\n",
            "Epoch 197/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 2.2493e-04 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 5.9391e-04 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.4859 - accuracy: 0.9889\n",
            "Epoch 200/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0395 - accuracy: 0.9889\n",
            "(10, 28, 28)\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 26.6124 - accuracy: 0.5000\n",
            "\n",
            "Test accuracy: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_vat = keras.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28,1)),  # Assuming input images are 28x28\n",
        "    layers.Dense(200, activation='relu'),   # Hidden layer with 200 neurons and ReLU activation\n",
        "    layers.Dense(10, activation='softmax')\n",
        "\n",
        "])\n",
        "\n",
        "def logit(x, is_training=True, update_batch_stats=True, stochastic=True, seed=1234):\n",
        "    return model_vat(labeled_x_train_subset)\n",
        "\n",
        "\n",
        "def forward(x, is_training=True, update_batch_stats=True, seed=1234):\n",
        "    if is_training:\n",
        "        return logit(x, is_training=True,\n",
        "                     update_batch_stats=update_batch_stats,\n",
        "                     stochastic=True, seed=seed)\n",
        "    else:\n",
        "        return logit(x, is_training=False,\n",
        "                     update_batch_stats=update_batch_stats,\n",
        "                     stochastic=False, seed=seed)\n",
        "\n",
        "\n",
        "def logsoftmax(x):\n",
        "    xdev = x - tf.reduce_max(x, 1, keep_dims=True)\n",
        "    lsm = xdev - tf.log(tf.reduce_sum(tf.exp(xdev), 1, keep_dims=True))\n",
        "    return lsm\n",
        "\n",
        "\n",
        "def kl_divergence_with_logit(q_logit, p_logit):\n",
        "    q = tf.nn.softmax(q_logit)\n",
        "    qlogq = tf.reduce_mean(tf.reduce_sum(q * logsoftmax(q_logit), 1))\n",
        "    qlogp = tf.reduce_mean(tf.reduce_sum(q * logsoftmax(p_logit), 1))\n",
        "    return qlogq - qlogp\n",
        "\n",
        "\n",
        "def get_normalized_vector(d):\n",
        "    d /= (1e-12 + tf.reduce_max(tf.abs(d), range(1, len(d.get_shape())), keep_dims=True))\n",
        "    d /= tf.sqrt(1e-6 + tf.reduce_sum(tf.pow(d, 2.0), range(1, len(d.get_shape())), keep_dims=True))\n",
        "    return d\n",
        "\n",
        "def generate_virtual_adversarial_perturbation(x, logit, is_training=True):\n",
        "    d = tf.random_normal(shape=tf.shape(x))\n",
        "\n",
        "    for _ in range(1):\n",
        "        d = 1e-6 * get_normalized_vector(d)\n",
        "        logit_p = logit\n",
        "        logit_m = forward(x + d, update_batch_stats=False, is_training=is_training)\n",
        "        dist = kl_divergence_with_logit(logit_p, logit_m)\n",
        "        grad = tf.gradients(dist, [d], aggregation_method=2)[0]\n",
        "        d = tf.stop_gradient(grad)\n",
        "\n",
        "    return 8.0 * get_normalized_vector(d)\n",
        "\n",
        "\n",
        "\n",
        "def virtual_adversarial_loss(x, logit, is_training=True, name=\"vat_loss\"):\n",
        "    r_vadv = generate_virtual_adversarial_perturbation(x, logit, is_training=is_training)\n",
        "    logit = tf.stop_gradient(logit)\n",
        "    print(logit.shape,'akfjdkfah')\n",
        "    logit_p = logit\n",
        "    logit_m = forward(x + r_vadv, update_batch_stats=False, is_training=is_training)\n",
        "    loss = kl_divergence_with_logit(logit_p, logit_m)\n",
        "    print(loss,'akfjdkfhadjk')\n",
        "    return tf.identity(loss, name=name)\n",
        "\n",
        "def ce_loss(logit, y):\n",
        "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
        "\n",
        "def build_training_graph(x, y, ul_x, lr):\n",
        "    logit = forward(x)\n",
        "    nll_loss = ce_loss(logit, y)\n",
        "    ul_logit = forward(ul_x, is_training=True, update_batch_stats=False)\n",
        "    vat_loss = virtual_adversarial_loss(ul_x, ul_logit)\n",
        "    additional_loss = vat_loss\n",
        "    loss = nll_loss + additional_loss\n",
        "\n",
        "    opt = tf.train.AdamOptimizer(learning_rate=lr)\n",
        "    tvars = tf.trainable_variables()\n",
        "    grads_and_vars = opt.compute_gradients(loss, tvars)\n",
        "    train_op = opt.apply_gradients(grads_and_vars, global_step=5)\n",
        "    return loss, train_op, global_step\n",
        "\n",
        "\n",
        "def accuracy(logit, y):\n",
        "    pred = tf.argmax(logit, 1)\n",
        "    true = tf.argmax(y, 1)\n",
        "    return tf.reduce_mean(tf.to_float(tf.equal(pred, true)))\n",
        "\n",
        "def build_eval_graph(x, y, ul_x):\n",
        "    losses = {}\n",
        "    logit = forward(x, is_training=False, update_batch_stats=False)\n",
        "    nll_loss = ce_loss(logit, y)\n",
        "    losses['NLL'] = nll_loss\n",
        "    acc = accuracy(logit, y)\n",
        "    losses['Acc'] = acc\n",
        "    scope = tf.get_variable_scope()\n",
        "    scope.reuse_variables()\n",
        "    # at_loss = adversarial_loss(x, y, nll_loss, is_training=False)\n",
        "    # losses['AT_loss'] = at_loss\n",
        "    ul_logit = forward(ul_x, is_training=False, update_batch_stats=False)\n",
        "    vat_loss = virtual_adversarial_loss(ul_x, ul_logit, is_training=False)\n",
        "    losses['VAT_loss'] = vat_loss\n",
        "    return losses\n",
        "\n",
        "\n",
        "# lr = tf.placeholder(tf.float32, shape=[], name=\"learning_rate\")\n",
        "# mom = tf.placeholder(tf.float32, shape=[], name=\"momentum\")\n",
        "loss, train_op, global_step = build_training_graph(labeled_x_train_subset, labeled_y_train_subset, unlabeled_dataset_x, 0.01)\n",
        "# Build eval graph\n",
        "losses_eval_train = build_eval_graph(images_eval_train, labels_eval_train, unlabeled_dataset_x)\n",
        "losses_eval_test = build_eval_graph(images_eval_test, labels_eval_test, images_eval_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "XfhMFIfH9U9R",
        "outputId": "14789f33-a31b-4c74-cc22-7b6844e81635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-8825fc5db88d>\u001b[0m in \u001b[0;36m<cell line: 108>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;31m# lr = tf.placeholder(tf.float32, shape=[], name=\"learning_rate\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;31m# mom = tf.placeholder(tf.float32, shape=[], name=\"momentum\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_training_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeled_x_train_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabeled_y_train_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlabeled_dataset_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;31m# Build eval graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0mlosses_eval_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_eval_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_eval_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_eval_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlabeled_dataset_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-8825fc5db88d>\u001b[0m in \u001b[0;36mbuild_training_graph\u001b[0;34m(x, y, ul_x, lr)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mnll_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mce_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mul_logit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mul_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_batch_stats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mvat_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvirtual_adversarial_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mul_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mul_logit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-8825fc5db88d>\u001b[0m in \u001b[0;36mce_loss\u001b[0;34m(logit, y)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mce_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_training_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mul_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5886\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5887\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5888\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__SoftmaxCrossEntropyWithLogits_device_/job:localhost/replica:0/task:0/device:CPU:0}} logits and labels must be broadcastable: logits_size=[80,10] labels_size=[1,80] [Op:SoftmaxCrossEntropyWithLogits] name: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kzh4-D0K_LkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Virtual adversarial training-formatted\n",
        "\n",
        "# Considering m samples., labeled_x_train_subset =\n",
        "# print(labeled_x_train_subset.shape, 'input dimension', labeled_x_train_subset.ndim)\n",
        "# generate random unit vector\n",
        "x = labeled_x_train_subset\n",
        "# generate random unit vector\n",
        "def generate_random_unit_vector(input):\n",
        "    x = np.random.normal(0, 1, input.shape)\n",
        "    d = x / np.linalg.norm(x)\n",
        "    return tf.Variable(d)\n",
        "\n",
        "# random_unit_vector = generate_random_unit_vector(labeled_x_train_subset)\n",
        "\n",
        "\n",
        "\n",
        "# Take input x, perturb input x with noise,we need to first find gaussian perturbation/noise\n",
        "\n",
        "r = tf.random.normal(shape=tf.shape(labeled_x_train_subset))\n",
        "r =  generate_random_unit_vector(r)\n",
        "# print(r.shape, 'perturbation')\n",
        "\n",
        "print(\"Shape of labeled input\", labeled_x_train_subset.shape, 'after unit normalized perturbation', r.shape)\n",
        "\n",
        "p_logit_r = model(labeled_x_train_subset+10*r)\n",
        "p_logit = model(labeled_x_train_subset)\n",
        "\n",
        "# print(p_logit.shape, p_logit_r.shape, 'Once model is traine by perturbed and original')\n",
        "kl_divergence = KLDivergence()\n",
        "# compare probability of perturbed input , original input\n",
        "# result1 = kl_divergence(tf.nn.softmax(p_logit), tf.nn.softmax(p_logit_r))\n",
        "# result = kl_divergence(p_logit, p_logit_r)\n",
        "# kl = tf.reduce_mean(result)\n",
        "\n",
        "assert p_logit.dtype in [tf.float32, tf.float64], \"Invalid data type for p_logit\"\n",
        "assert p_logit_r.dtype in [tf.float32, tf.float64], \"Invalid data type for p_logit_r\"\n",
        "\n",
        "assert p_logit.shape == p_logit_r.shape, \"Shapes of p_logit and p_logit_r should match\"\n",
        "\n",
        "assert isinstance(p_logit, (tf.Tensor, tf.Variable)), \"p_logit should be a TensorFlow tensor or variable\"\n",
        "assert isinstance(p_logit_r, (tf.Tensor, tf.Variable)), \"p_logit_r should be a TensorFlow tensor or variable\"\n",
        "# assert isinstance(r, (tf.Tensor, tf.Variable)), \"r should be a TensorFlow tensor or variable\"\n",
        "\n",
        "# Using tf.GradientTape to compute gradients\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "   result = kl_divergence(p_logit, p_logit_r)\n",
        "   print(\"Result\", result)\n",
        "   kl = tf.reduce_mean(result)\n",
        "\n",
        "grad_kl = tape.gradient(kl, [r])[0]\n",
        "\n",
        "\n",
        "\n",
        "print(grad_kl,'dakfjdajkfha')\n",
        "\n",
        "r_vadv = tf.stop_gradient(grad_kl)\n",
        "\n",
        "r_vadv = generate_random_unit_vector( r_vadv )/3.0\n",
        "\n",
        "\n",
        "r_vadv_expanded = tf.expand_dims(r_vadv, axis=1)\n",
        "\n",
        "print(r_vadv_expanded.shape, r_vadv.shape, labeled_x_train_subset.shape)\n",
        "# p_logit_r_adv = model(labeled_x_train_subset  + r_vadv_expanded )\n",
        "# vat_loss =  tf.reduce_mean(kl_divergence( tf.stop_gradient(p_logit), p_logit_r_adv ))\n",
        "\n",
        "# model_vat = model(labeled_x_train_subset , p )\n",
        "# model_vat.add_loss( vat_loss   )\n",
        "# model_vat.compile( 'sgd' ,  'categorical_crossentropy'  ,  metrics=['accuracy'])\n",
        "\n",
        "# model.fit( labeled_x_train_subset , train_labels_y )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "kBsu6qRBsNgs",
        "outputId": "838dc090-47ee-4fad-88f7-51eb75ba91f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of labeled input (80, 28, 28) after unit normalized perturbation (80, 28, 28)\n",
            "Result tf.Tensor(2.286253e-10, shape=(), dtype=float32)\n",
            "None dakfjdajkfha\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-bfc0f78fd601>\u001b[0m in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_kl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'dakfjdajkfha'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mr_vadv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_kl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mr_vadv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_random_unit_vector\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mr_vadv\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = labeled_x_train_subset\n",
        "\n",
        "def generate_random_unit_vector(input):\n",
        "    x = np.random.normal(0, 1, input.shape)\n",
        "    d = x / np.linalg.norm(x)\n",
        "    return d\n",
        "\n",
        "\n",
        "r = tf.random.normal(shape=tf.shape(labeled_x_train_subset))\n",
        "# r =  generate_random_unit_vector(r)\n",
        "perturbed_input = labeled_x_train_subset+0.01*r\n",
        "\n",
        "model_vat = keras.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28,1)),  # Assuming input images are 28x28\n",
        "    layers.Dense(200, activation='relu'),   # Hidden layer with 200 neurons and ReLU activation\n",
        "    layers.Dense(10, activation='softmax')\n",
        "\n",
        "])\n",
        "\n",
        "alpha = 1.0  # A hyperparameter for controlling the strength of the perturbation\n",
        "optimizer = keras.optimizers.Adam()\n",
        "model_vat.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "def virtual_adversarial_loss(x, logits):\n",
        "    d = generate_random_unit_vector(x)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(x)\n",
        "        logits_perturbed = model_vat(x + alpha * d)\n",
        "        loss = tf.keras.losses.categorical_crossentropy(logits, logits_perturbed)\n",
        "\n",
        "    r_vadv = tape.gradient(loss, x)\n",
        "    r_vadv_normalized = alpha * r_vadv / tf.norm(r_vadv)\n",
        "\n",
        "    logits_perturbed = model_vat(x + r_vadv_normalized)\n",
        "    loss = tf.keras.losses.categorical_crossentropy(logits, logits_perturbed)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i in range(0, len(perturbed_input), batch_size):\n",
        "        x_batch = perturbed_input[i:i+batch_size]\n",
        "        y_batch = train_labels_y[i:i+batch_size]\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model_vat(x_batch)\n",
        "            classification_loss = tf.keras.losses.categorical_crossentropy(y_batch, logits)\n",
        "            vat_loss = virtual_adversarial_loss(x_batch, logits)\n",
        "            total_loss = classification_loss + vat_loss\n",
        "        gradients = tape.gradient(total_loss, model_vat.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model_vat.trainable_variables))\n",
        "\n",
        "    if i % 40 == 0:\n",
        "              mean_total_loss = tf.reduce_mean(total_loss).numpy()\n",
        "              # print(mean_total_loss, )\n",
        "              print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/80] Total Loss: {mean_total_loss:.4f}')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model_vat.evaluate(labeled_x_test_subset, test_labels_y)\n",
        "print(\"Test accuracy for VAT:\", test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVKj5jlQANhP",
        "outputId": "5f04a7fc-09c6-4935-dce8-5c2bbcd5b34c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Step [81/80] Total Loss: 79.7432\n",
            "Epoch [2/20], Step [81/80] Total Loss: 43.9595\n",
            "Epoch [3/20], Step [81/80] Total Loss: 29.7828\n",
            "Epoch [4/20], Step [81/80] Total Loss: 17.9712\n",
            "Epoch [5/20], Step [81/80] Total Loss: 3.6936\n",
            "Epoch [6/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [7/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [8/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [9/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [10/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [11/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [12/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [13/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [14/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [15/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [16/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [17/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [18/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [19/20], Step [81/80] Total Loss: 0.0000\n",
            "Epoch [20/20], Step [81/80] Total Loss: 0.0000\n",
            "1/1 [==============================] - 0s 348ms/step - loss: 62.2365 - accuracy: 0.5000\n",
            "Test accuracy for VAT: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Virtual adversarial training -original\n",
        "\n",
        "# Considering m samples., labeled_x_train_subset =\n",
        "print(labeled_x_train_subset.shape, 'input dimension', labeled_x_train_subset.ndim)\n",
        "# generate random unit vector\n",
        "x = labeled_x_train_subset\n",
        "# generate random unit vector\n",
        "def generate_random_unit_vector(input):\n",
        "    x = np.random.normal(0, 1, input.shape)\n",
        "    d = x / np.linalg.norm(x)\n",
        "    return d\n",
        "\n",
        "# random_unit_vector = generate_random_unit_vector(labeled_x_train_subset)\n",
        "\n",
        "\n",
        "\n",
        "# Take input x, perturb input x with noise,we need to first find gaussian perturbation/noise\n",
        "\n",
        "r = tf.random.normal(shape=tf.shape(labeled_x_train_subset))\n",
        "r =  generate_random_unit_vector(r)\n",
        "# print(r.shape, 'perturbation')\n",
        "\n",
        "epsilon = 1e-16\n",
        "print(norm.shape)\n",
        "# r = r / (tf.reshape(norm, [-1, 1]) + epsilon)\n",
        "\n",
        "# r = r/(tf.reshape(tf.sqrt(tf.reduce_sum(tf.pow(r, 2.0), axis=1)), [-1, 1]) + 1e-16)\n",
        "p_logit_r = model(labeled_x_train_subset+10*r)\n",
        "p_logit = model(labeled_x_train_subset)\n",
        "kl_divergence = KLDivergence()\n",
        "# compare probability of perturbed input , original input\n",
        "# result = kl_divergence(tf.nn.softmax(p_logit), tf.nn.softmax(p_logit_r))\n",
        "result = kl_divergence(p_logit, p_logit_r)\n",
        "kl = tf.reduce_mean(result)\n",
        "\n",
        "x = tf.constant(3.0)\n",
        "with tf.GradientTape() as g:\n",
        "  g.watch(x)\n",
        "  y = x * x\n",
        "\n",
        "dy_dx = g.gradient(y, x)\n",
        "print(dy_dx)\n",
        "\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    # 1. Compute KL divergence\n",
        "    tape.watch(r)\n",
        "    result = kl_divergence(p_logit, p_logit_r)\n",
        "    print(result)\n",
        "    kl = tf.reduce_mean(result)\n",
        "\n",
        "\n",
        "\n",
        "grad_kl = tape.gradient( kl , [r])[0]\n",
        "\n",
        "print(grad_kl,'dakfjdajkfha')\n",
        "\n",
        "# r_vadv = tf.stop_gradient(grad_kl)\n",
        "# r_vadv = make_unit_norm( r_vadv )/3.0\n",
        "# p_logit_r_adv = model( labeled_x_train_subset  + r_vadv )\n",
        "# vat_loss =  tf.reduce_mean(kl_divergence( tf.stop_gradient(p_logit), p_logit_r_adv ))\n",
        "\n",
        "# model_vat = model(labeled_x_train_subset , p )\n",
        "# model_vat.add_loss( vat_loss   )\n",
        "# model_vat.compile( 'sgd' ,  'categorical_crossentropy'  ,  metrics=['accuracy'])\n",
        "\n",
        "# model.fit( labeled_x_train_subset , train_labels_y )\n"
      ],
      "metadata": {
        "id": "LccYhmHEH4jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entropy minimization\n",
        "# Log likelihood of labelled data\n",
        "\n",
        "# labeled_x_train_subset, train_labels_y\n",
        "\n",
        "# Just for checking\n",
        "# print(train_labels_y[0])\n",
        "# image = labeled_x_train_subset[0]\n",
        "# # Display the image\n",
        "# plt.imshow(image, cmap='gray')\n",
        "# plt.title(f\"Label: {train_labels_y[0]}\")\n",
        "# plt.show()\n",
        "# just for checking single input\n",
        "# single_input = np.reshape(labeled_x_test_subset[0], (1, 28, 28))\n",
        "# prediction = model.predict(single_input)\n",
        "# print(np.argmax(prediction))\n",
        "# LOG LIKELIHOOD OF LABELLED DATA\n",
        "# dot product of labeled one hot vector & prediction and then finding log of each data and then finding sum.\n",
        "\n",
        "# predictions_y = model.predict(labeled_x_train_subset)\n",
        "# print(predictions_y)\n",
        "# dot_product_of_one_hot_vector_predictions = np.dot(train_labels_y.T, predictions_y)\n",
        "# epsilon = 1e-10\n",
        "# dot_product_result = np.maximum(dot_product_of_one_hot_vector_predictions, epsilon)\n",
        "# log_result = np.log(dot_product_result)\n",
        "# result = np.sum(log_result, axis=1)\n",
        "# print(\"HKJFDHAKJFHDSJKFHAJSFHEUWHUI\", log_result, result)\n",
        "\n",
        "\n",
        "# sum_of_all_categories_for_ith_data = np.sum(np.log(train_labels_y.T, predictions_y))\n",
        "# print(sum_of_all_categories_for_ith_data)\n",
        "# max_predictions_value = np.argmax(predictions)\n",
        "# print(max_predictions_value,'fjahjfhdalfj')\n",
        "# print(predictions_y.shape, np.dot(train_labels_y.T, predictions_y).shape,  np.log(np.dot(train_labels_y.T, predictions_y)))\n",
        "# output_dot_product = np.dot(train_labels_y.T, predictions_y)\n",
        "# print(output_dot_product.shape,'fkhdsakjfhdajklsh')\n",
        "\n",
        "\n",
        "def entropy_minimization_loss(y_true, y_pred, alpha=0.001):\n",
        "    # Calculating the standard cross-entropy loss\n",
        "    print(\"INSIDE ENTROPY MINIMIZATION LOSS\", y_true, y_pred)\n",
        "    cross_entropy_loss = categorical_crossentropy(y_true, y_pred)\n",
        "    print(cross_entropy_loss.shape)\n",
        "\n",
        "\n",
        "    # Calculating the entropy of the predicted probabilities\n",
        "    entropy = tf.reduce_sum(-y_pred * tf.math.log(y_pred), axis=1)\n",
        "    print('afdkafjk', entropy)\n",
        "\n",
        "    # Combining the cross-entropy loss with the entropy regularization term\n",
        "    total_loss = cross_entropy_loss + alpha * entropy\n",
        "\n",
        "    # tf.debugging.check_numerics(cross_entropy_loss, \"cross_entropy_loss is NaN or Inf\")\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "\n",
        "model_entropy_minimization = keras.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28,1)),  # Assuming input images are 28x28\n",
        "    layers.Dense(200, activation='relu'),   # Hidden layer with 200 neurons and ReLU activation\n",
        "    layers.Dense(10, activation='softmax')\n",
        "\n",
        "])\n",
        "model_entropy_minimization.compile(optimizer='adam', loss=entropy_minimization_loss, metrics=['accuracy'])\n",
        "model_entropy_minimization.fit(labeled_x_train_subset, train_labels_y, epochs=20, batch_size=64)\n",
        "\n",
        "test_loss, test_acc = model_entropy_minimization.evaluate(labeled_x_test_subset, test_labels_y)\n",
        "print(\"Test accuracy for entropy minimization\", test_acc)\n",
        "\n",
        "# optimizer = Adam(clipvalue=1.0)  # You can adjust the clip value\n",
        "# model.compile(optimizer=optimizer, loss=entropy_minimization_loss, metrics=['accuracy'])\n",
        "# model.compile(optimizer='adam', loss=entropy_minimization_loss, metrics=['accuracy'])\n",
        "\n",
        "print(\"NaNs in input data:\", np.isnan(labeled_x_train_subset).any())\n",
        "print(\"Infinities in input data:\", np.isinf(labeled_x_train_subset).any())\n",
        "\n",
        "# Check for NaN or infinite values in labels\n",
        "print(\"NaNs in labels:\", np.isnan(train_labels_y).any())\n",
        "print(\"Infinities in labels:\", np.isinf(train_labels_y).any())\n",
        "\n",
        "# model.fit(labeled_x_train_subset, train_labels_y, epochs=20, batch_size=64)\n",
        "\n",
        "\n",
        "\n",
        "# predictions_y = model.predict(labeled_x_train_subset)\n",
        "# dot_product_of_one_hot_vector_predictions = np.dot(train_labels_y.T, predictions_y)\n",
        "# epsilon = 1e-10\n",
        "# dot_product_result = np.maximum(dot_product_of_one_hot_vector_predictions, epsilon)\n",
        "# log_result = np.log(dot_product_result)\n",
        "# result = np.sum(log_result, axis=1)\n",
        "\n",
        "\n",
        "# For unlabelled data\n",
        "# unlabeled_datas_y = np.ones((9900,10))\n",
        "# predictions_unlabeled_y = model.predict(unlabeled_dataset_x)\n",
        "\n",
        "# test_loss, test_acc = model.evaluate(unlabeled_dataset_x, predictions_unlabeled_y)\n",
        "# print('test accuracy', test_acc)\n",
        "\n",
        "# print(predictions_unlabeled_y.shape, 'shape of unlabeled')\n",
        "# log_of_each_predictions = np.log(predictions_unlabeled_y)\n",
        "# print('afdkjashfdjahfjdhfajhdjfd', log_of_each_predictions)\n",
        "# print(labeled_x_test_subset.shape)\n",
        "# Evaluate the model on the test set\n",
        "# print(log_of_each_predictions.shape)\n",
        "# test_loss, test_acc = model.evaluate(unlabeled_dataset_x, log_of_each_predictions)\n",
        "# print('test accuracy', test_acc)\n",
        "\n",
        "# epsilon = 1e-10\n",
        "# lambda_params = 0.1\n",
        "# log_of_each_predictions_result = np.maximum(log_of_each_predictions, epsilon)\n",
        "\n",
        "\n",
        "# dot_product_of_log_predicted_and_predicted = np.dot(predictions_unlabeled_y.T, log_of_each_predictions_result)\n",
        "# print('kafkjhdahfjhfjkahfja', log_of_each_predictions_result.shape, predictions_unlabeled_y.shape, dot_product_of_log_predicted_and_predicted.shape)\n",
        "# result_unlabelled = np.sum(dot_product_of_log_predicted_and_predicted, axis=1)\n",
        "# print('Final result for unlabeled', result_unlabelled*lambda_params)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slig_rjjzWx-",
        "outputId": "dc504206-0aec-4476-b036-1e41b4c00c59"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "INSIDE ENTROPY MINIMIZATION LOSS Tensor(\"IteratorGetNext:1\", shape=(None, 10), dtype=float32) Tensor(\"sequential_15/dense_31/Softmax:0\", shape=(None, 10), dtype=float32)\n",
            "(None,)\n",
            "afdkafjk Tensor(\"entropy_minimization_loss/Sum:0\", shape=(None,), dtype=float32)\n",
            "INSIDE ENTROPY MINIMIZATION LOSS Tensor(\"IteratorGetNext:1\", shape=(None, 10), dtype=float32) Tensor(\"sequential_15/dense_31/Softmax:0\", shape=(None, 10), dtype=float32)\n",
            "(None,)\n",
            "afdkafjk Tensor(\"entropy_minimization_loss/Sum:0\", shape=(None,), dtype=float32)\n",
            "2/2 [==============================] - 1s 19ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 2/20\n",
            "2/2 [==============================] - 0s 20ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 3/20\n",
            "2/2 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 4/20\n",
            "2/2 [==============================] - 0s 23ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 5/20\n",
            "2/2 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 6/20\n",
            "2/2 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 7/20\n",
            "2/2 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 8/20\n",
            "2/2 [==============================] - 0s 17ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 9/20\n",
            "2/2 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 10/20\n",
            "2/2 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 11/20\n",
            "2/2 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 12/20\n",
            "2/2 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 13/20\n",
            "2/2 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 14/20\n",
            "2/2 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 15/20\n",
            "2/2 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 16/20\n",
            "2/2 [==============================] - 0s 21ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 17/20\n",
            "2/2 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 18/20\n",
            "2/2 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 19/20\n",
            "2/2 [==============================] - 0s 17ms/step - loss: nan - accuracy: 0.1000\n",
            "Epoch 20/20\n",
            "2/2 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.1000\n",
            "INSIDE ENTROPY MINIMIZATION LOSS Tensor(\"IteratorGetNext:1\", shape=(None, 10), dtype=float32) Tensor(\"sequential_15/dense_31/Softmax:0\", shape=(None, 10), dtype=float32)\n",
            "(None,)\n",
            "afdkafjk Tensor(\"entropy_minimization_loss/Sum:0\", shape=(None,), dtype=float32)\n",
            "1/1 [==============================] - 0s 252ms/step - loss: nan - accuracy: 0.1000\n",
            "Test accuracy for entropy minimization 0.10000000149011612\n",
            "NaNs in input data: False\n",
            "Infinities in input data: False\n",
            "NaNs in labels: False\n",
            "Infinities in labels: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pseudo labels:\n",
        "pseudo_label_predictions = model.predict(unlabeled_dataset_x)\n",
        "max_predictions_value = np.argmax(pseudo_label_predictions)\n",
        "\n",
        "\n",
        "\n",
        "train_labels_y_value = keras.utils.to_categorical(labeled_dataset_to_be_trained_y, num_classes=10)\n",
        "#  combine labeled & unlabeled dataset:\n",
        "new_train_x = np.vstack((labeled_dataset_to_be_trained_x, unlabeled_dataset_x))\n",
        "new_train_y = np.vstack((train_labels_y_value, pseudo_label_predictions))\n",
        "\n",
        "\n",
        "print(labeled_dataset_to_be_trained_x.shape, unlabeled_dataset_x.shape, new_train_x.shape)\n",
        "print(train_labels_y.shape, pseudo_label_predictions.shape,new_train_y.shape)\n",
        "\n",
        "# train the model with this dataset\n",
        "labeled_x_train_subset_pseudo,labeled_x_test_subset_pseudo, labeled_y_train_subset_pseudo, labeled_y_test_subset_pseudo = train_test_split(\n",
        "    new_train_x, new_train_y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# train_labels_y_pseudo = keras.utils.to_categorical(labeled_y_train_subset_pseudo, num_classes=10)\n",
        "# test_labels_y_pseudo = keras.utils.to_categorical(labeled_y_test_subset_pseudo, num_classes=10)\n",
        "\n",
        "# Extract true labels for the labeled subset\n",
        "train_labels_y_pseudo = labeled_y_train_subset_pseudo[:, :10]\n",
        "test_labels_y_pseudo = labeled_y_test_subset_pseudo[:, :10]\n",
        "\n",
        "print(train_labels_y_pseudo.shape, test_labels_y_pseudo.shape, labeled_x_train_subset_pseudo.shape)\n",
        "# Train the model\n",
        "model.fit(labeled_x_train_subset_pseudo, train_labels_y_pseudo , epochs=20, batch_size=64)\n",
        "# model.fit(labeled_x_train_subset_pseudo, train_labels_y_pseudo , epochs=20, batch_size=64, validation_data=(labeled_x_test_subset_pseudo, test_labels_y_pseudo))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(labeled_x_test_subset_pseudo, test_labels_y_pseudo)\n",
        "print('\\nTest accuracy:', test_acc)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f06GRcr_LYA",
        "outputId": "ed1eca4e-28ac-45f8-a756-f182143596a5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "310/310 [==============================] - 1s 3ms/step\n",
            "98969 fjahjfhdalfj\n",
            "(9900, 10) 98969\n",
            "(100, 28, 28) (9900, 28, 28) (10000, 28, 28)\n",
            "(80, 10) (9900, 10) (10000, 10)\n",
            "(8000, 10) (2000, 10) (8000, 28, 28)\n",
            "Epoch 1/20\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 5.7006 - accuracy: 0.6386\n",
            "Epoch 2/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 1.6521 - accuracy: 0.5790\n",
            "Epoch 3/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 1.3484 - accuracy: 0.6061\n",
            "Epoch 4/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 1.2512 - accuracy: 0.6314\n",
            "Epoch 5/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 1.1232 - accuracy: 0.6411\n",
            "Epoch 6/20\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.0580 - accuracy: 0.6706\n",
            "Epoch 7/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 1.0285 - accuracy: 0.6726\n",
            "Epoch 8/20\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.0239 - accuracy: 0.6756\n",
            "Epoch 9/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.9614 - accuracy: 0.6865\n",
            "Epoch 10/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.9215 - accuracy: 0.7081\n",
            "Epoch 11/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.8959 - accuracy: 0.7084\n",
            "Epoch 12/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.8993 - accuracy: 0.7010\n",
            "Epoch 13/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.8831 - accuracy: 0.7171\n",
            "Epoch 14/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.8703 - accuracy: 0.7199\n",
            "Epoch 15/20\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.8389 - accuracy: 0.7232\n",
            "Epoch 16/20\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.8322 - accuracy: 0.7271\n",
            "Epoch 17/20\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.8320 - accuracy: 0.7369\n",
            "Epoch 18/20\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.8197 - accuracy: 0.7346\n",
            "Epoch 19/20\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.8214 - accuracy: 0.7381\n",
            "Epoch 20/20\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.8086 - accuracy: 0.7513\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.8035 - accuracy: 0.8040\n",
            "\n",
            "Test accuracy: 0.8040000200271606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VATLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, model, xi=1e-6, epsilon=1.0, ip=1):\n",
        "        super(VATLoss, self).__init__()\n",
        "        self.model = model\n",
        "        self.xi = xi\n",
        "        self.epsilon = epsilon\n",
        "        self.ip = ip\n",
        "\n",
        "    def call(self, x, training=None):\n",
        "        # Generate a random perturbation\n",
        "        d = tf.random.normal(shape=tf.shape(x))\n",
        "\n",
        "        # Normalize the perturbation\n",
        "        d = self._l2_normalize(d)\n",
        "\n",
        "\n",
        "        # Calculate the perturbed input\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(x)\n",
        "            r_adv = self._get_virtual_adversarial_perturbation(x, d)\n",
        "\n",
        "        # Forward pass on the perturbed input\n",
        "        y_pred_adv = self.model(x + r_adv, training=training)\n",
        "\n",
        "        # Calculate KL divergence loss between original and perturbed predictions\n",
        "        loss = self._kl_divergence_loss(y_pred_adv, self.model(x, training=training))\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def _kl_divergence_loss(self, pred1, pred2):\n",
        "        log_softmax_pred1 = tf.nn.log_softmax(pred1)\n",
        "        softmax_pred2 = tf.nn.softmax(pred2)\n",
        "        kl_loss = tf.reduce_mean(tf.reduce_sum(log_softmax_pred1 * softmax_pred2, axis=-1))\n",
        "        return kl_loss\n",
        "\n",
        "    def _l2_normalize(self, d):\n",
        "        d /= tf.sqrt(tf.reduce_sum(tf.square(d), axis=(1, 2), keepdims=True) + 1e-8)\n",
        "        return d\n",
        "\n",
        "    def _get_virtual_adversarial_perturbation(self, x, d):\n",
        "        for _ in range(self.ip):\n",
        "            with tf.GradientTape() as tape:\n",
        "                tape.watch(d)\n",
        "                y_pred = self.model(x + d, training=True)\n",
        "                kl_loss = self._kl_divergence_loss(y_pred, self.model(x, training=True))\n",
        "\n",
        "            grad = tape.gradient(kl_loss, d)\n",
        "            d = self._l2_normalize(grad)\n",
        "\n",
        "        r_adv = self.epsilon * d\n",
        "        return r_adv\n",
        "\n",
        "# Example usage:\n",
        "# Assuming you have a neural network model defined (e.g., MyModel) and a dataset (e.g., DataLoader)\n",
        "\n",
        "# Instantiate the model and VATLoss\n",
        "model_vat = keras.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28,1)),  # Assuming input images are 28x28\n",
        "    layers.Dense(200, activation='relu'),   # Hidden layer with 200 neurons and ReLU activation\n",
        "    layers.Dense(10, activation='softmax')\n",
        "\n",
        "])\n",
        "# model_vat.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# model_vat.fit(perturbed_input, train_labels_y, epochs=20, batch_size=64)\n",
        "vat_loss = VATLoss(model_vat)\n",
        "\n",
        "# Define your optimizer\n",
        "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, clipvalue=0.5)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 30\n",
        "for epoch in range(num_epochs):\n",
        "    for step in range(80):\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass\n",
        "            outputs = model_vat(labeled_x_train_subset, training=True)\n",
        "            classification_loss = tf.keras.losses.categorical_crossentropy(train_labels_y, outputs)\n",
        "\n",
        "            # Virtual Adversarial Training loss\n",
        "            # print(tf.constant(labeled_x_train_subset))\n",
        "            vat_loss_value = vat_loss(tf.constant(labeled_x_train_subset, dtype=tf.float32), outputs)\n",
        "\n",
        "            # Total loss\n",
        "            total_loss = classification_loss + vat_loss_value\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        gradients = tape.gradient(total_loss, model_vat.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model_vat.trainable_variables))\n",
        "\n",
        "        # Print training statistics\n",
        "        if step % 40 == 0:\n",
        "              mean_total_loss = tf.reduce_mean(total_loss).numpy()\n",
        "              # print(mean_total_loss, )\n",
        "              print(f'Epoch [{epoch+1}/{num_epochs}], Step [{step+1}/80] Total Loss: {mean_total_loss:.4f}')\n",
        "\n",
        "\n",
        "def calculate_accuracy(model, inputs, labels):\n",
        "    predictions = model(inputs, training=False)\n",
        "    predicted_labels = tf.argmax(predictions, axis=1)\n",
        "    true_labels = tf.argmax(labels, axis=1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted_labels, true_labels), tf.float32))\n",
        "    return accuracy.numpy()\n",
        "\n",
        "\n",
        "# print(unlabeled_dataset_x.shape)\n",
        "# for step in range(9900):\n",
        "#           pseudo_labels = model_vat(unlabeled_dataset_x, training=False)\n",
        "\n",
        "\n",
        "# test_accuracy = calculate_accuracy(model_vat, unlabeled_dataset_x, pseudo_labels)\n",
        "# print(f'Test Accuracy: {test_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckLhV8I78zcZ",
        "outputId": "9bebf062-b033-4715-892d-212bbd1a9c69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<keras.src.engine.sequential.Sequential object at 0x79a348d87130> akjfdkajfak\n",
            "Epoch [1/30], Step [1/80] Total Loss: 147.1577\n",
            "Epoch [1/30], Step [41/80] Total Loss: -2.2292\n",
            "Epoch [2/30], Step [1/80] Total Loss: -2.2292\n",
            "Epoch [2/30], Step [41/80] Total Loss: -2.2292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xu1YKATO05LW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}